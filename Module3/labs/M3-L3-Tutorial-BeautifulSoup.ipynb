{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advance Data Parsing | XML / HTML / KML / etc.\n",
    "---\n",
    "\n",
    "Beautiful Soup is a very powerful HTML/XML parser library for Python. The Beautiful Soup website provides the best description of its use-case:  \n",
    "\n",
    "\"You didn't write that awful page. You're just trying to get some data out of it. Beautiful Soup is here to help. Since 2004, it's been saving programmers hours or days of work on quick-turnaround screen scraping projects.\"  \n",
    "\n",
    "We will be using Beautiful Soup to extract the data we want out of various XML-like files that might have some data we don't need, or as a convenient way to work with the data that is in an other-wise unruly format.\n",
    "\n",
    "For a deep understanding of Beautiful Soup, you will need both experience using it and to have read the documentation in its entirety. The full documentation can be found here:  \n",
    "\n",
    "https://www.crummy.com/software/BeautifulSoup/bs4/doc/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping HTML\n",
    "--- \n",
    "\n",
    "We will start by looking at how to explore an HTML page to extract an unordered list. \n",
    "\n",
    "Then we will look at a parsing a simple HTML table into a pandas data frame.  \n",
    "\n",
    "Finally we will begin, and leave the finishing for an exercise, looking at an example of using Beautiful Soup to scrape data from an HTML table on a web page. We will have some code in this example to facilitate downloading the HTML page, and pulling some information out of the table on that page. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### An Unordered List  \n",
    "---\n",
    "\n",
    "This example contains a very simple list, showing some of the initial basic syntax needed for Beautiful Soup.\n",
    "\n",
    "Source: http://www.w3schools.com/html/html_lists.asp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Defining an HTML String\n",
    "html = \"\"\"\n",
    "<ul style=\"list-style-type:circle\">\n",
    "  <li>Coffee</li>\n",
    "  <li>Tea</li>\n",
    "  <li>Milk</li>\n",
    "</ul>\"\"\"\n",
    "\n",
    "# Parse the file into Soup\n",
    "soup = BeautifulSoup(html, \"html\")\n",
    "# If you uncomment the below line, you will see that \n",
    "# Beautiful Soup will automatically try to fix\n",
    "# some of the HTML for us. \n",
    "\n",
    "# print(soup)\n",
    "\n",
    "drinks = []\n",
    "\n",
    "for item in soup.findAll('li'):\n",
    "    drinks.append(item.text)\n",
    "    \n",
    "print(drinks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A Simple Table\n",
    "---\n",
    "The next example involves pulling the data out of a fairly simple HTML table. For simplicity, we will \"prepare\" the data for a pandas data frame, but won't actually instantiate a data frame at this time.\n",
    "\n",
    "Table Source: http://www.w3schools.com/html/html_tables.asp\n",
    "\n",
    "<span style=\"background:yellow;\">**NOTE**: As you read through this example please un-comment and re-comment various `print()` statements.</span>   \n",
    "Keep re-running the cell as you do to study the stages of the data and objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from pprint import pprint\n",
    "import pandas as pd\n",
    "\n",
    "# THE simulated HTML table data\n",
    "html = \"\"\"\n",
    "<!DOCTYPE html>\n",
    "<html>\n",
    "<head>\n",
    "<style>\n",
    "table {\n",
    "    font-family: arial, sans-serif;\n",
    "    border-collapse: collapse;\n",
    "    width: 100%;\n",
    "}\n",
    "\n",
    "td, th {\n",
    "    border: 1px solid #dddddd;\n",
    "    text-align: left;\n",
    "    padding: 8px;\n",
    "}\n",
    "\n",
    "tr:nth-child(even) {\n",
    "    background-color: #dddddd;\n",
    "}\n",
    "</style>\n",
    "</head>\n",
    "<body>\n",
    "\n",
    "<table>\n",
    "  <tr>\n",
    "    <th>Company</th>\n",
    "    <th>Contact</th>\n",
    "    <th>Country</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Alfreds Futterkiste</td>\n",
    "    <td>Maria Anders</td>\n",
    "    <td>Germany</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Centro comercial Moctezuma</td>\n",
    "    <td>Francisco Chang</td>\n",
    "    <td>Mexico</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Ernst Handel</td>\n",
    "    <td>Roland Mendel</td>\n",
    "    <td>Austria</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Island Trading</td>\n",
    "    <td>Helen Bennett</td>\n",
    "    <td>UK</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Laughing Bacchus Winecellars</td>\n",
    "    <td>Yoshi Tannamuri</td>\n",
    "    <td>Canada</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Magazzini Alimentari Riuniti</td>\n",
    "    <td>Giovanni Rovelli</td>\n",
    "    <td>Italy</td>\n",
    "  </tr>\n",
    "</table>\n",
    "\n",
    "</body>\n",
    "</html>\"\"\"\n",
    "\n",
    "soup = BeautifulSoup(html, \"html\")\n",
    "# print(soup.table) # Just to show how easy it is to extract the table from the full page. \n",
    "\n",
    "table = soup.table\n",
    "# Pull out the column names\n",
    "# We know we want the first row, thus we can do the following:\n",
    "\n",
    "col_names = []\n",
    "for th_cell in table.tr.findAll('th'):\n",
    "    # print(th_cell.string)\n",
    "    col_names.append(th_cell.string)\n",
    "\n",
    "# print(col_names) # To see the column names we extracted\n",
    "\n",
    "# This is a dictionary comprehension.\n",
    "# It creates a dictionary of key:value pairs\n",
    "# The keys are the column names we extracted earlier\n",
    "# \n",
    "data = {key: [] for key in col_names}\n",
    "\n",
    "# print(data) # To see the dictionary we just made.\n",
    "\n",
    "for row in table.findAll('tr')[1:]: #Pull all of the rows, and disregard the first one.\n",
    "    data_cells = row.findAll('td')\n",
    "    data['Company'].append(data_cells[0].string)\n",
    "    data['Contact'].append(data_cells[1].string)\n",
    "    data['Country'].append(data_cells[2].string)\n",
    "\n",
    "# pprint(data) # Pretty print the dictionary.\n",
    "\n",
    "pd.DataFrame(data) #Now we have a data frame to use!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A Live Web Page with Real Data\n",
    "---\n",
    "\n",
    "For the last, and pretty intensive, challenge, you should complete the code below. \n",
    "\n",
    "What this code snippet does is download a webpage from a live website which contains a table we want to ultimately use in a data frame. The code necessary for downloading the HTML has been provided for you.  \n",
    "\n",
    "Your challenge is to read through the provided code, play with it uncommenting, and changing it to explore Beautiful Soup and the data we are using. I have left my thoughts and various \"development\" statements in the code, so you can see what a \"typical\" development process is like for this sort of task. The ultimate goal is to take the HTML table and get the data into a pandas data frame so that we can easily manipulate it there.  \n",
    "\n",
    "This is a relatively intensive and challenging task, so give yourself some time and don't get discouraged. \n",
    "\n",
    "\n",
    "For those new to inspecting web pages: [How to Inspect a Web  Page Source for Scraper Development](https://web.dsa.missouri.edu/static/PDF/AnalyzingHTMLwithTheWebInspector.pdf)\n",
    "\n",
    "\n",
    "<span style=\"background:yellow;\">**NOTE**: AGAIN! ... As you read through this example please un-comment and re-comment various `print()` statements.</span>   \n",
    "Keep re-running the cell as you do to study the stages of the data and objects.\n",
    "\n",
    "\n",
    "<span style=\"background:yellow;\">**NOTE**: We created an easier version of the webpage to use for parsing.  To try the more difficult check the commented url code to switch.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from pprint import pprint\n",
    "\n",
    "# --------- BEGIN GET THE HTML CONTENT ------------\n",
    "# ----The below url is a easier version of an actual webpage ---- #\n",
    "url = 'https://indigo.sgn.missouri.edu/static/mirror_sites/www.basketball-reference.com/draft/basketballdata.html'\n",
    "# ----The below url is an actual webpage but the table is more difficult to parse ---- #\n",
    "#url = 'https://indigo.sgn.missouri.edu/static/mirror_sites/www.basketball-reference.com/draft/NBA_2015.html'\n",
    "r = requests.get(url)\n",
    "\n",
    "# check the status_code to make sure we retrieved the HTML successfully.\n",
    "# print(r.status_code)\n",
    "# print(r.content) # Uncomment this line to see the raw source.\n",
    "\n",
    "soup = BeautifulSoup(r.content, \"html\")\n",
    "# print(soup) # uncomment this line to see how BS4 cleans up the HTML for us. \n",
    "# --------- END GET THE HTML CONTENT ------------\n",
    "\n",
    "# --------- BEGIN BS4 DATA EXPLORATION ------------\n",
    "\n",
    "# We know from quick inspection of the webpage in-browser that \n",
    "# the table we want has an id of 'stats'\n",
    "\n",
    "table = soup.find(id='stats')\n",
    "# Since there is only one table on the page, \n",
    "# All of the below would are equivalent.\n",
    "# soup.table == soup.find('table') == soup.find(id='stats')\n",
    "\n",
    "# We use this to explore the table.\n",
    "# The code retrieves the second row from the table.\n",
    "header_row = table.find_all('tr')[1]\n",
    "#print(header_row)\n",
    "\n",
    "# After reviewing the above row, you'll notice that the \"aria-label\" attribute is\n",
    "# a good candidate for DataFrame column names. So, let's pull those into an array\n",
    "\n",
    "names = []\n",
    "\n",
    "# With exploration, we discover that we want the first 16 aria-labels exactly how they are,\n",
    "#but will need to process the data-tip attribute for the others\n",
    "\n",
    "# NOTE: This is not the simplest way to do this. \n",
    "# I chose to use the limit parameter to show it's use,\n",
    "# since this section is on Beautiful Soup.\n",
    "# Adding all of the tips and using slicing might be clearer to some. \n",
    "\n",
    "for each in header_row.findAll('th', limit=20):\n",
    "    # print(each.attrs['aria-label']) # to see the label to verify we grabbed what we wanted.\n",
    "    names.append(each.attrs['aria-label'])\n",
    "\n",
    "# print(names) # Again, just confirming we got what we wanted.\n",
    "\n",
    "# We find all of the tags the have a tip attribute,\n",
    "# And then we use a \"slice\" to only grab the ones we\n",
    "# have not already processed.\n",
    "\n",
    "for each in header_row.findAll('th')[20:]:\n",
    "    # This is a good time to point out that data carpentry is usually\n",
    "    # not pretty. If not for Beautiful Soup, the *entire* script would be \n",
    "    # like this.\n",
    "    # print(each.attrs['data-tip'])\n",
    "    names.append(each.attrs['data-tip'].split('><')[0][3:-3]) \n",
    "\n",
    "# print(names) # If we want to verify that the names were extracted properly\n",
    "\n",
    "\n",
    "# --------- BEGIN PULLING DATA OUT OF TABLE INTO DICTIONARY ------------\n",
    "# --- Finish the code below this line ---\n",
    "pprint(names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Save Your Notebook"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
