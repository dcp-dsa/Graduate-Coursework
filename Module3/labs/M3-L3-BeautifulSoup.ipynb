{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advance Data Parsing | XML / HTML / KML / etc.\n",
    "---\n",
    "\n",
    "Beautiful Soup is a very powerful HTML/XML parser library for Python. The Beautiful Soup website provides the best description of its use-case:  \n",
    "\n",
    "\"You didn't write that awful page. You're just trying to get some data out of it. Beautiful Soup is here to help. Since 2004, it's been saving programmers hours or days of work on quick-turnaround screen scraping projects.\"  \n",
    "\n",
    "We will be using Beautiful Soup to extract the data we want out of various XML-like files that might have some data we don't need, or as a convenient way to work with the data that is in an other-wise unruly format.\n",
    "\n",
    "For a deep understanding of Beautiful Soup, you will need both experience using it and to have read the documentation in its entirety. The full documentation can be found here:  \n",
    "\n",
    "https://www.crummy.com/software/BeautifulSoup/bs4/doc/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping HTML\n",
    "--- \n",
    "\n",
    "We will start by looking at how to explore an HTML page to extract an unordered list. \n",
    "\n",
    "Then we will look at a parsing a simple HTML table into a pandas data frame.  \n",
    "\n",
    "Finally we will begin, and leave the finishing for an exercise, looking at an example of using Beautiful Soup to scrape data from an HTML table on a web page. We will have some code in this example to facilitate downloading the HTML page, and pulling some information out of the table on that page. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### An Unordered List  \n",
    "---\n",
    "\n",
    "This example contains a very simple list, showing some of the initial basic syntax needed for Beautiful Soup.\n",
    "\n",
    "Source: http://www.w3schools.com/html/html_lists.asp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Coffee', 'Tea', 'Milk']\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Defining an HTML String\n",
    "html = \"\"\"\n",
    "<ul style=\"list-style-type:circle\">\n",
    "  <li>Coffee</li>\n",
    "  <li>Tea</li>\n",
    "  <li>Milk</li>\n",
    "</ul>\"\"\"\n",
    "\n",
    "# Parse the file into Soup\n",
    "soup = BeautifulSoup(html, \"html\")\n",
    "# If you uncomment the below line, you will see that \n",
    "# Beautiful Soup will automatically try to fix\n",
    "# some of the HTML for us. \n",
    "\n",
    "# print(soup)\n",
    "\n",
    "drinks = []\n",
    "\n",
    "for item in soup.findAll('li'):\n",
    "    drinks.append(item.text)\n",
    "    \n",
    "print(drinks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A Simple Table\n",
    "---\n",
    "The next example involves pulling the data out of a fairly simple HTML table. For simplicity, we will \"prepare\" the data for a pandas data frame, but won't actually instantiate a data frame at this time.\n",
    "\n",
    "Table Source: http://www.w3schools.com/html/html_tables.asp\n",
    "\n",
    "<span style=\"background:yellow;\">**NOTE**: As you read through this example please un-comment and re-comment various `print()` statements.</span>   \n",
    "Keep re-running the cell as you do to study the stages of the data and objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Company</th>\n",
       "      <th>Contact</th>\n",
       "      <th>Country</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Alfreds Futterkiste</td>\n",
       "      <td>Maria Anders</td>\n",
       "      <td>Germany</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Centro comercial Moctezuma</td>\n",
       "      <td>Francisco Chang</td>\n",
       "      <td>Mexico</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ernst Handel</td>\n",
       "      <td>Roland Mendel</td>\n",
       "      <td>Austria</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Island Trading</td>\n",
       "      <td>Helen Bennett</td>\n",
       "      <td>UK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Laughing Bacchus Winecellars</td>\n",
       "      <td>Yoshi Tannamuri</td>\n",
       "      <td>Canada</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Magazzini Alimentari Riuniti</td>\n",
       "      <td>Giovanni Rovelli</td>\n",
       "      <td>Italy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        Company           Contact  Country\n",
       "0           Alfreds Futterkiste      Maria Anders  Germany\n",
       "1    Centro comercial Moctezuma   Francisco Chang   Mexico\n",
       "2                  Ernst Handel     Roland Mendel  Austria\n",
       "3                Island Trading     Helen Bennett       UK\n",
       "4  Laughing Bacchus Winecellars   Yoshi Tannamuri   Canada\n",
       "5  Magazzini Alimentari Riuniti  Giovanni Rovelli    Italy"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from pprint import pprint\n",
    "import pandas as pd\n",
    "\n",
    "# THE simulated HTML table data\n",
    "html = \"\"\"\n",
    "<!DOCTYPE html>\n",
    "<html>\n",
    "<head>\n",
    "<style>\n",
    "table {\n",
    "    font-family: arial, sans-serif;\n",
    "    border-collapse: collapse;\n",
    "    width: 100%;\n",
    "}\n",
    "\n",
    "td, th {\n",
    "    border: 1px solid #dddddd;\n",
    "    text-align: left;\n",
    "    padding: 8px;\n",
    "}\n",
    "\n",
    "tr:nth-child(even) {\n",
    "    background-color: #dddddd;\n",
    "}\n",
    "</style>\n",
    "</head>\n",
    "<body>\n",
    "\n",
    "<table>\n",
    "  <tr>\n",
    "    <th>Company</th>\n",
    "    <th>Contact</th>\n",
    "    <th>Country</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Alfreds Futterkiste</td>\n",
    "    <td>Maria Anders</td>\n",
    "    <td>Germany</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Centro comercial Moctezuma</td>\n",
    "    <td>Francisco Chang</td>\n",
    "    <td>Mexico</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Ernst Handel</td>\n",
    "    <td>Roland Mendel</td>\n",
    "    <td>Austria</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Island Trading</td>\n",
    "    <td>Helen Bennett</td>\n",
    "    <td>UK</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Laughing Bacchus Winecellars</td>\n",
    "    <td>Yoshi Tannamuri</td>\n",
    "    <td>Canada</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Magazzini Alimentari Riuniti</td>\n",
    "    <td>Giovanni Rovelli</td>\n",
    "    <td>Italy</td>\n",
    "  </tr>\n",
    "</table>\n",
    "\n",
    "</body>\n",
    "</html>\"\"\"\n",
    "\n",
    "soup = BeautifulSoup(html, \"html\")\n",
    "# print(soup.table) # Just to show how easy it is to extract the table from the full page. \n",
    "\n",
    "table = soup.table\n",
    "# Pull out the column names\n",
    "# We know we want the first row, thus we can do the following:\n",
    "\n",
    "col_names = []\n",
    "for th_cell in table.tr.findAll('th'):\n",
    "    # print(th_cell.string)\n",
    "    col_names.append(th_cell.string)\n",
    "\n",
    "# print(col_names) # To see the column names we extracted\n",
    "\n",
    "# This is a dictionary comprehension.\n",
    "# It creates a dictionary of key:value pairs\n",
    "# The keys are the column names we extracted earlier\n",
    "# \n",
    "data = {key: [] for key in col_names}\n",
    "\n",
    "# print(data) # To see the dictionary we just made.\n",
    "\n",
    "for row in table.findAll('tr')[1:]: #Pull all of the rows, and disregard the first one.\n",
    "    data_cells = row.findAll('td')\n",
    "    data['Company'].append(data_cells[0].string)\n",
    "    data['Contact'].append(data_cells[1].string)\n",
    "    data['Country'].append(data_cells[2].string)\n",
    "\n",
    "# pprint(data) # Pretty print the dictionary.\n",
    "\n",
    "pd.DataFrame(data) #Now we have a data frame to use!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A Live Web Page with Real Data\n",
    "---\n",
    "\n",
    "For the last, and pretty intensive, challenge, you should complete the code below. \n",
    "\n",
    "What this code snippet does is download a webpage from a live website which contains a table we want to ultimately use in a data frame. The code necessary for downloading the HTML has been provided for you.  \n",
    "\n",
    "Your challenge is to read through the provided code, play with it uncommenting, and changing it to explore Beautiful Soup and the data we are using. I have left my thoughts and various \"development\" statements in the code, so you can see what a \"typical\" development process is like for this sort of task. The ultimate goal is to take the HTML table and get the data into a pandas data frame so that we can easily manipulate it there.  \n",
    "\n",
    "This is a relatively intensive and challenging task, so give yourself some time and don't get discouraged. \n",
    "\n",
    "\n",
    "For those new to inspecting web pages: [How to Inspect a Web  Page Source for Scraper Development](https://web.dsa.missouri.edu/static/PDF/AnalyzingHTMLwithTheWebInspector.pdf)\n",
    "\n",
    "\n",
    "<span style=\"background:yellow;\">**NOTE**: AGAIN! ... As you read through this example please un-comment and re-comment various `print()` statements.</span>   \n",
    "Keep re-running the cell as you do to study the stages of the data and objects.\n",
    "\n",
    "\n",
    "<span style=\"background:yellow;\">**NOTE**: We created an easier version of the webpage to use for parsing.  To try the more difficult check the commented url code to switch.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Rank',\n",
      " 'Overall Pick',\n",
      " 'Team',\n",
      " 'Player',\n",
      " 'College',\n",
      " 'Seasons that player has appeared in the NBA',\n",
      " 'Games',\n",
      " 'Minutes Played',\n",
      " 'Points',\n",
      " 'Total Rebounds',\n",
      " 'Assists',\n",
      " 'Field Goal Percentage',\n",
      " '3-Point Field Goal Percentage',\n",
      " 'Free Throw Percentage',\n",
      " 'Minutes Played Per Game',\n",
      " 'Points Per Game',\n",
      " 'Total Rebounds Per Game',\n",
      " 'Assists Per Game',\n",
      " 'Win Shares',\n",
      " 'Win Shares Per 48 Minutes',\n",
      " 'Box Plus/Minus',\n",
      " 'Value over Replacement Player']\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from pprint import pprint\n",
    "\n",
    "# --------- BEGIN GET THE HTML CONTENT ------------\n",
    "# ----The below url is a easier version of an actual webpage ---- #\n",
    "url = 'https://indigo.sgn.missouri.edu/static/mirror_sites/www.basketball-reference.com/draft/basketballdata.html'\n",
    "# ----The below url is an actual webpage but the table is more difficult to parse ---- #\n",
    "#url = 'https://indigo.sgn.missouri.edu/static/mirror_sites/www.basketball-reference.com/draft/NBA_2015.html'\n",
    "r = requests.get(url)\n",
    "\n",
    "# check the status_code to make sure we retrieved the HTML successfully.\n",
    "# print(r.status_code)\n",
    "# print(r.content) # Uncomment this line to see the raw source.\n",
    "\n",
    "soup = BeautifulSoup(r.content, \"html\")\n",
    "# print(soup) # uncomment this line to see how BS4 cleans up the HTML for us. \n",
    "# --------- END GET THE HTML CONTENT ------------\n",
    "\n",
    "# --------- BEGIN BS4 DATA EXPLORATION ------------\n",
    "\n",
    "# We know from quick inspection of the webpage in-browser that \n",
    "# the table we want has an id of 'stats'\n",
    "\n",
    "table = soup.find(id='stats')\n",
    "# Since there is only one table on the page, \n",
    "# All of the below would are equivalent.\n",
    "# soup.table == soup.find('table') == soup.find(id='stats')\n",
    "\n",
    "# We use this to explore the table.\n",
    "# The code retrieves the second row from the table.\n",
    "header_row = table.find_all('tr')[1]\n",
    "#print(header_row)\n",
    "\n",
    "# After reviewing the above row, you'll notice that the \"aria-label\" attribute is\n",
    "# a good candidate for DataFrame column names. So, let's pull those into an array\n",
    "\n",
    "names = []\n",
    "\n",
    "# With exploration, we discover that we want the first 16 aria-labels exactly how they are,\n",
    "#but will need to process the data-tip attribute for the others\n",
    "\n",
    "# NOTE: This is not the simplest way to do this. \n",
    "# I chose to use the limit parameter to show it's use,\n",
    "# since this section is on Beautiful Soup.\n",
    "# Adding all of the tips and using slicing might be clearer to some. \n",
    "\n",
    "for each in header_row.findAll('th', limit=20):\n",
    "    # print(each.attrs['aria-label']) # to see the label to verify we grabbed what we wanted.\n",
    "    names.append(each.attrs['aria-label'])\n",
    "\n",
    "# print(names) # Again, just confirming we got what we wanted.\n",
    "\n",
    "# We find all of the tags the have a tip attribute,\n",
    "# And then we use a \"slice\" to only grab the ones we\n",
    "# have not already processed.\n",
    "\n",
    "for each in header_row.findAll('th')[20:]:\n",
    "    # This is a good time to point out that data carpentry is usually\n",
    "    # not pretty. If not for Beautiful Soup, the *entire* script would be \n",
    "    # like this.\n",
    "    # print(each.attrs['data-tip'])\n",
    "    names.append(each.attrs['data-tip'].split('><')[0][3:-3]) \n",
    "\n",
    "# print(names) # If we want to verify that the names were extracted properly\n",
    "\n",
    "\n",
    "# --------- BEGIN PULLING DATA OUT OF TABLE INTO DICTIONARY ------------\n",
    "# --- Finish the code below this line ---\n",
    "pprint(names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Now that the column names are pulled out into their own array, there are a number of ways to get the data out of the HTML table and into a data structure that can then be used to create a data frame. Refer to previous examples and the pandas documentation if needed. \n",
    "\n",
    "In a practice at the end of the day, you will work on a more challenging cell. \n",
    "The key is to string the various concepts you have been learning together."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KML Example \n",
    "---\n",
    "The below code is pasted from canvas so that you have the opportunity to run it and see what it produces. \n",
    "Feel free to edit the code below to further explore the KML file, and for even more practice with Beautiful Soup.\n",
    "\n",
    "#### Note, for KML we will install an enhanced parser for XML, which KML is a specialization of.\n",
    "\n",
    "You will need to restart your notebook kernel (`Kernel > Restart`) after the next command completes!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata: done\n",
      "Solving environment: \\ \n",
      "The environment is inconsistent, please check the package plan carefully\n",
      "The following packages are causing the inconsistency:\n",
      "\n",
      "  - conda-forge/linux-64::matplotlib==3.0.3=py37_1\n",
      "- "
     ]
    }
   ],
   "source": [
    "!conda install -y lxml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "FeatureNotFound",
     "evalue": "Couldn't find a tree builder with the features you requested: lxml-xml. Do you need to install a parser library?",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFeatureNotFound\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-2ece1bfbb188>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Parse the file object into a XML Structured Document, AKA : KML Document Object Model (DOM)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/dsa/data/all_datasets/geonode-Syria_IDPSites_2015LateJun_HIU_DoS.kml'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Latin-1'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mkml_file_object\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mdom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkml_file_object\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'lxml-xml'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mpm\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfindAll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Placemark'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/bs4/__init__.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, markup, features, builder, parse_only, from_encoding, exclude_encodings, **kwargs)\u001b[0m\n\u001b[1;32m    194\u001b[0m                     \u001b[0;34m\"Couldn't find a tree builder with the features you \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m                     \u001b[0;34m\"requested: %s. Do you need to install a parser library?\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m                     % \",\".join(features))\n\u001b[0m\u001b[1;32m    197\u001b[0m             \u001b[0mbuilder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuilder_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m             if not (original_features == builder.NAME or\n",
      "\u001b[0;31mFeatureNotFound\u001b[0m: Couldn't find a tree builder with the features you requested: lxml-xml. Do you need to install a parser library?"
     ]
    }
   ],
   "source": [
    "# This is copied from the canvas site so that you now have a chance to run it.\n",
    "# Feel free to add / change the code to familiarize yourself with Beautiful Soup 4.\n",
    "\n",
    "# Beautiful Soup\n",
    "from bs4 import BeautifulSoup\n",
    " \n",
    "# Open the file as a file object\n",
    "# Parse the file object into a XML Structured Document, AKA : KML Document Object Model (DOM)\n",
    "with open('/dsa/data/all_datasets/geonode-Syria_IDPSites_2015LateJun_HIU_DoS.kml', encoding='Latin-1') as kml_file_object:\n",
    "    dom = BeautifulSoup(kml_file_object,'lxml-xml')\n",
    "    \n",
    "for pm in dom.findAll('Placemark'):\n",
    "    for name in pm.findAll('name'):\n",
    "        nametext = name.find(text = True)\n",
    "        print(nametext)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Save Your Notebook"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
