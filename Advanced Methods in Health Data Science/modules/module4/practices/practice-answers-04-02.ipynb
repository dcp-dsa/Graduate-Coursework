{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 4 Practice 2 Answers - CNN modification\n",
    "\n",
    "## **NOTE: You need to use the Tensorflow CPU container for this notebook**\n",
    "(It may take a while for this container to start and for your notebook to connect to a kernel, so be patient)\n",
    "\n",
    "In this practice exercise you will modify the CNN that was built in the lab.  You will add and alter layers in the CNN to examine the effect on the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install keras==2.3.1\n",
    "!{sys.executable} -m pip install --upgrade \"numpy>=1.2\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import math\n",
    "import glob\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten, BatchNormalization, Conv2D, MaxPooling2D\n",
    "from keras.optimizers import Adadelta, SGD\n",
    "from keras.utils import np_utils\n",
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
    "\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, roc_curve, auc, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "# we will set a random seed to exert some control over the random processes that occur in the neural network training.\n",
    "tf.set_random_seed(42) \n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create functions from the lab\n",
    "These functions are from the lab and we'll define them in their original form first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle(X, y):\n",
    "    # shuffle the images in a random order so similar images and labeled classes are not grouped together\n",
    "    rng = np.random.default_rng(seed=42)\n",
    "    perm = rng.permutation(len(y))\n",
    "    X = X[perm]\n",
    "    y = y[perm]\n",
    "    print (np.shape(X))\n",
    "    return X, y\n",
    "\n",
    "\n",
    "def read_image_data(filename):\n",
    "    # read grayscale image data to an 2d numpy array\n",
    "    image = Image.open(filename)\n",
    "    image = image.getdata()\n",
    "    image = np.array(image)\n",
    "    return image.reshape(-1)\n",
    "\n",
    "\n",
    "def image_dir_to_array(dir):\n",
    "    data = [read_image_data(image) for image in glob.glob(os.path.join(dir, '*.jpg'))]\n",
    "    return np.array(data)\n",
    "\n",
    "\n",
    "def load_data(negative_images_path, positive_images_path):\n",
    "    negatives = image_dir_to_array(negative_images_path)\n",
    "    positives = image_dir_to_array(positive_images_path)\n",
    "    \n",
    "    X=np.vstack((negatives, positives))\n",
    "    X=X.astype(np.float) / 255 # reduce colordepth normalize the image grayscale values from 0..1\n",
    "    y=np.concatenate((np.zeros(len(negatives)), np.ones(len(positives))))\n",
    "    \n",
    "    print ('shape of X', np.shape(X)) \n",
    "    print ('scale of X', np.min(X), np.max(X))\n",
    "    print ('shape of y', np.shape(y)) \n",
    "    \n",
    "    X, y = shuffle(X, y)\n",
    "    return X, y\n",
    "\n",
    "\n",
    "def reshape_X(X, img_channels, img_rows, img_cols):\n",
    "    # reshape the data to the 4 dimensional format required by the CNN\n",
    "    # the resulting shape will be (num_samples, img_channels (1 for grayscale images), img_rows, img_cols)\n",
    "    return X.reshape(-1, img_channels, img_rows, img_cols)\n",
    "\n",
    "\n",
    "def step_decay(epoch):\n",
    "    initial_lrate = 0.01\n",
    "    drop = 0.3\n",
    "    epochs_drop = 30.0\n",
    "    lrate = initial_lrate * math.pow(drop, math.floor((1+epoch)/epochs_drop))\n",
    "    print ('learning rate', lrate)\n",
    "    return lrate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(img_channels, img_rows, img_cols):\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Conv2D(16, kernel_size = 3, padding='same', input_shape=(img_channels, img_rows, img_cols)))\n",
    "    model.add(Activation('relu'))\n",
    "\n",
    "    model.add(Conv2D(16, kernel_size = 5, padding=\"same\"))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    \n",
    "    model.add(MaxPooling2D(pool_size = 2, data_format='channels_first'))\n",
    "\n",
    "    model.add(Conv2D(16, kernel_size = 3, padding=\"same\"))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "\n",
    "    model.add(Conv2D(64, kernel_size = 5, padding=\"same\"))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "\n",
    "    model.add(Conv2D(64, kernel_size = 3, padding=\"same\"))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    \n",
    "    model.add(MaxPooling2D(pool_size = 2, data_format='channels_first'))\n",
    "\n",
    "    model.add(Conv2D(128, kernel_size = 3, padding=\"same\"))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "\n",
    "    model.add(MaxPooling2D(pool_size = 2, data_format='channels_first'))\n",
    "    \n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, kernel_initializer=\"he_normal\"))\n",
    "    model.add(Activation('relu'))\n",
    "\n",
    "    model.add(Dropout(0.5)) \n",
    "    model.add(Dense(32, kernel_initializer=\"he_normal\"))\n",
    "    model.add(Activation('relu'))\n",
    "\n",
    "    model.add(Dropout(0.5)) \n",
    "    model.add(Dense(1))\n",
    "    model.add(Activation('sigmoid'))\n",
    "    \n",
    "    # learning rate optimizer\n",
    "    optimizer = Adadelta(lr=0)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data and create a train test split\n",
    "Use a 33% test size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_rows=32\n",
    "img_cols=32\n",
    "img_channels=1\n",
    "\n",
    "negative_images_path = '../resources/cnn-images/negative_images/'\n",
    "positive_images_path = '../resources/cnn-images/positive_images/'\n",
    "\n",
    "X, y = load_data(negative_images_path, positive_images_path)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "\n",
    "X_train = reshape_X(X_train, img_channels, img_rows, img_cols)\n",
    "X_test = reshape_X(X_test, img_channels, img_rows, img_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recreate the original model\n",
    "Use the code from the lab to recreate the original model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_model(img_channels, img_rows, img_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the original model\n",
    "Use 10 epochs to train the model.  This should provide slightly more stable models, but will take twice as long to train.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=64\n",
    "nb_epoch=10\n",
    "\n",
    "# save our model at the end\n",
    "model_checkpoint = ModelCheckpoint('model.h5', verbose=1, monitor='val_loss', save_best_only=True)\n",
    "\n",
    "# create a learning rate callback\n",
    "learning_rate = LearningRateScheduler(step_decay)\n",
    "\n",
    "model.fit(X_train, y_train, batch_size=batch_size, nb_epoch=nb_epoch, validation_split=0.1,\n",
    "          callbacks=[model_checkpoint,learning_rate], shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the model\n",
    "Output the ROC value.  We will use the ROC to evaluate the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred = model.predict(X_test, batch_size = 32)\n",
    "roc = roc_auc_score(y_test, Y_pred)\n",
    "print(\"ROC:\", round(roc, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modify the model to remove the dropouts\n",
    "\n",
    "Create the model without the dropouts.  Train, and evalutate this new model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(img_channels, img_rows, img_cols):\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Conv2D(16, kernel_size = 3, padding='same', input_shape=(img_channels, img_rows, img_cols)))\n",
    "    model.add(Activation('relu'))\n",
    "\n",
    "    model.add(Conv2D(16, kernel_size = 5, padding=\"same\"))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    \n",
    "    model.add(MaxPooling2D(pool_size = 2, data_format='channels_first'))\n",
    "\n",
    "    model.add(Conv2D(16, kernel_size = 3, padding=\"same\"))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "\n",
    "    model.add(Conv2D(64, kernel_size = 5, padding=\"same\"))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "\n",
    "    model.add(Conv2D(64, kernel_size = 3, padding=\"same\"))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    \n",
    "    model.add(MaxPooling2D(pool_size = 2, data_format='channels_first'))\n",
    "\n",
    "    model.add(Conv2D(128, kernel_size = 3, padding=\"same\"))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "\n",
    "    model.add(MaxPooling2D(pool_size = 2, data_format='channels_first'))\n",
    "    \n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, kernel_initializer=\"he_normal\"))\n",
    "    model.add(Activation('relu'))\n",
    "\n",
    "#     model.add(Dropout(0.5)) # we could also set the rate to zero but it incurs additional processing for no reason\n",
    "    model.add(Dense(32, kernel_initializer=\"he_normal\"))\n",
    "    model.add(Activation('relu'))\n",
    "\n",
    "#     model.add(Dropout(0.5)) \n",
    "    model.add(Dense(1))\n",
    "    model.add(Activation('sigmoid'))\n",
    "    \n",
    "    # learning rate optimizer\n",
    "    optimizer = Adadelta(lr=0)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_model(img_channels, img_rows, img_cols)\n",
    "\n",
    "model.fit(X_train, y_train, batch_size=batch_size, nb_epoch=nb_epoch, validation_split=0.1,\n",
    "          callbacks=[model_checkpoint,learning_rate], shuffle=True)\n",
    "\n",
    "Y_pred = model.predict(X_test, batch_size = 32)\n",
    "roc = roc_auc_score(y_test, Y_pred)\n",
    "print(\"ROC:\", round(roc, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare\n",
    "Did the ROC increase?  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It probably did, and it's because we're likely overfitting the training data without the dropouts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modify the model\n",
    "Add back in the dropouts, and also add in a new convolution layer.\n",
    "\n",
    "After this code:\n",
    "```\n",
    "    model.add(Conv2D(128, kernel_size = 3, padding=\"same\"))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "\n",
    "```\n",
    "\n",
    "add the following code:\n",
    "```\n",
    "    model.add(Conv2D(128, kernel_size = 5, padding=\"same\"))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "```\n",
    "\n",
    "Recreate, retrain, and evaluate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(img_channels, img_rows, img_cols):\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Conv2D(16, kernel_size = 3, padding='same', input_shape=(img_channels, img_rows, img_cols)))\n",
    "    model.add(Activation('relu'))\n",
    "\n",
    "    model.add(Conv2D(16, kernel_size = 5, padding=\"same\"))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    \n",
    "    model.add(MaxPooling2D(pool_size = 2, data_format='channels_first'))\n",
    "\n",
    "    model.add(Conv2D(16, kernel_size = 3, padding=\"same\"))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "\n",
    "    model.add(Conv2D(64, kernel_size = 5, padding=\"same\"))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "\n",
    "    model.add(Conv2D(64, kernel_size = 3, padding=\"same\"))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    \n",
    "    model.add(MaxPooling2D(pool_size = 2, data_format='channels_first'))\n",
    "\n",
    "    model.add(Conv2D(128, kernel_size = 3, padding=\"same\"))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "\n",
    "    model.add(Conv2D(128, kernel_size = 5, padding=\"same\"))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "\n",
    "    model.add(MaxPooling2D(pool_size = 2, data_format='channels_first'))\n",
    "    \n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, kernel_initializer=\"he_normal\"))\n",
    "    model.add(Activation('relu'))\n",
    "\n",
    "    model.add(Dropout(0.5)) \n",
    "    model.add(Dense(32, kernel_initializer=\"he_normal\"))\n",
    "    model.add(Activation('relu'))\n",
    "\n",
    "    model.add(Dropout(0.5)) \n",
    "    model.add(Dense(1))\n",
    "    model.add(Activation('sigmoid'))\n",
    "    \n",
    "    # learning rate optimizer\n",
    "    optimizer = Adadelta(lr=0)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_model(img_channels, img_rows, img_cols)\n",
    "\n",
    "model.fit(X_train, y_train, batch_size=batch_size, nb_epoch=nb_epoch, validation_split=0.1,\n",
    "          callbacks=[model_checkpoint,learning_rate], shuffle=True)\n",
    "\n",
    "Y_pred = model.predict(X_test, batch_size = 32)\n",
    "roc = roc_auc_score(y_test, Y_pred)\n",
    "print(\"ROC:\", round(roc, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Did this new layer improve the model over the original (first) model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last model is likely to have improved the ROC over the original model, and possibly rivaled the overfit model without the Dropouts.  With a very low number of epochs, it's possible that the model fluctuated enough that this wasn't the case when you ran the notebook, but running the final model multiple times should produce a better ROC than the first model.  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
