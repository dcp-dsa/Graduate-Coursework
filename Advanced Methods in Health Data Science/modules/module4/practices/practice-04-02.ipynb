{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 4 Practice 2 - CNN modification\n",
    "\n",
    "## **NOTE: You need to use the Tensorflow CPU container for this notebook**\n",
    "\n",
    "In this practice exercise you will modify the CNN that was built in the lab.  You will add and alter layers in the CNN to examine the effect on the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: keras==2.3.1 in /opt/conda/lib/python3.7/site-packages (2.3.1)\n",
      "Requirement already satisfied: h5py in /opt/conda/lib/python3.7/site-packages (from keras==2.3.1) (2.9.0)\n",
      "Requirement already satisfied: scipy>=0.14 in /opt/conda/lib/python3.7/site-packages (from keras==2.3.1) (1.2.1)\n",
      "Requirement already satisfied: keras-preprocessing>=1.0.5 in /opt/conda/lib/python3.7/site-packages (from keras==2.3.1) (1.0.9)\n",
      "Requirement already satisfied: pyyaml in /opt/conda/lib/python3.7/site-packages (from keras==2.3.1) (5.1.2)\n",
      "Requirement already satisfied: keras-applications>=1.0.6 in /opt/conda/lib/python3.7/site-packages (from keras==2.3.1) (1.0.7)\n",
      "Requirement already satisfied: numpy>=1.9.1 in /opt/conda/lib/python3.7/site-packages (from keras==2.3.1) (1.21.5)\n",
      "Requirement already satisfied: six>=1.9.0 in /opt/conda/lib/python3.7/site-packages (from keras==2.3.1) (1.12.0)\n",
      "Requirement already up-to-date: numpy>=1.2 in /opt/conda/lib/python3.7/site-packages (1.21.5)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install keras==2.3.1\n",
    "!{sys.executable} -m pip install --upgrade \"numpy>=1.2\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import math\n",
    "import glob\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten, BatchNormalization, Conv2D, MaxPooling2D\n",
    "from keras.optimizers import Adadelta, SGD\n",
    "from keras.utils import np_utils\n",
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
    "\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, roc_curve, auc, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "# we will set a random seed to exert some control over the random processes that occur in the neural network training.\n",
    "tf.set_random_seed(42) \n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create functions from the lab\n",
    "These functions are from the lab and we'll define them in their original form first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle(X, y):\n",
    "    # shuffle the images in a random order so similar images and labeled classes are not grouped together\n",
    "    rng = np.random.default_rng(seed=42)\n",
    "    perm = rng.permutation(len(y))\n",
    "    X = X[perm]\n",
    "    y = y[perm]\n",
    "    print (np.shape(X))\n",
    "    return X, y\n",
    "\n",
    "\n",
    "def read_image_data(filename):\n",
    "    # read grayscale image data to an 2d numpy array\n",
    "    image = Image.open(filename)\n",
    "    image = image.getdata()\n",
    "    image = np.array(image)\n",
    "    return image.reshape(-1)\n",
    "\n",
    "\n",
    "def image_dir_to_array(dir):\n",
    "    data = [read_image_data(image) for image in glob.glob(os.path.join(dir, '*.jpg'))]\n",
    "    return np.array(data)\n",
    "\n",
    "\n",
    "def load_data(negative_images_path, positive_images_path):\n",
    "    negatives = image_dir_to_array(negative_images_path)\n",
    "    positives = image_dir_to_array(positive_images_path)\n",
    "    \n",
    "    X=np.vstack((negatives, positives))\n",
    "    X=X.astype(np.float) / 255 # reduce colordepth normalize the image grayscale values from 0..1\n",
    "    y=np.concatenate((np.zeros(len(negatives)), np.ones(len(positives))))\n",
    "    \n",
    "    print ('shape of X', np.shape(X)) \n",
    "    print ('scale of X', np.min(X), np.max(X))\n",
    "    print ('shape of y', np.shape(y)) \n",
    "    \n",
    "    X, y = shuffle(X, y)\n",
    "    return X, y\n",
    "\n",
    "\n",
    "def reshape_X(X, img_channels, img_rows, img_cols):\n",
    "    # reshape the data to the 4 dimensional format required by the CNN\n",
    "    # the resulting shape will be (num_samples, img_channels (1 for grayscale images), img_rows, img_cols)\n",
    "    return X.reshape(-1, img_channels, img_rows, img_cols)\n",
    "\n",
    "\n",
    "def step_decay(epoch):\n",
    "    initial_lrate = 0.01\n",
    "    drop = 0.3\n",
    "    epochs_drop = 30.0\n",
    "    lrate = initial_lrate * math.pow(drop, math.floor((1+epoch)/epochs_drop))\n",
    "    print ('learning rate', lrate)\n",
    "    return lrate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(img_channels, img_rows, img_cols):\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Conv2D(16, kernel_size = 3, padding='same', input_shape=(img_channels, img_rows, img_cols)))\n",
    "    model.add(Activation('relu'))\n",
    "\n",
    "    model.add(Conv2D(16, kernel_size = 5, padding=\"same\"))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    \n",
    "    model.add(MaxPooling2D(pool_size = 2, data_format='channels_first'))\n",
    "\n",
    "    model.add(Conv2D(16, kernel_size = 3, padding=\"same\"))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "\n",
    "    model.add(Conv2D(64, kernel_size = 5, padding=\"same\"))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "\n",
    "    model.add(Conv2D(64, kernel_size = 3, padding=\"same\"))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    \n",
    "    model.add(MaxPooling2D(pool_size = 2, data_format='channels_first'))\n",
    "\n",
    "    model.add(Conv2D(128, kernel_size = 3, padding=\"same\"))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "\n",
    "    model.add(MaxPooling2D(pool_size = 2, data_format='channels_first'))\n",
    "    \n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, kernel_initializer=\"he_normal\"))\n",
    "    model.add(Activation('relu'))\n",
    "\n",
    "    model.add(Dropout(0.5)) \n",
    "    model.add(Dense(32, kernel_initializer=\"he_normal\"))\n",
    "    model.add(Activation('relu'))\n",
    "\n",
    "    model.add(Dropout(0.5)) \n",
    "    model.add(Dense(1))\n",
    "    model.add(Activation('sigmoid'))\n",
    "    \n",
    "    # learning rate optimizer\n",
    "    optimizer = Adadelta(lr=0)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data and create a train test split\n",
    "Use a 33% test size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of X (8710, 1024)\n",
      "scale of X 0.0 1.0\n",
      "shape of y (8710,)\n",
      "(8710, 1024)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:29: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n"
     ]
    }
   ],
   "source": [
    "# your code here\n",
    "\n",
    "img_rows=32\n",
    "img_cols=32\n",
    "img_channels=1\n",
    "\n",
    "negative_images_path = '../resources/cnn-images/negative_images/'\n",
    "positive_images_path = '../resources/cnn-images/positive_images/'\n",
    "\n",
    "X, y = load_data(negative_images_path, positive_images_path)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "\n",
    "X_train = reshape_X(X_train, img_channels, img_rows, img_cols)\n",
    "X_test = reshape_X(X_test, img_channels, img_rows, img_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recreate the original model\n",
    "Use the code from the lab to recreate the original model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "# your code here\n",
    "\n",
    "model = create_model(img_channels, img_rows, img_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the original model\n",
    "Use 10 epochs to train the model.  This should provide slightly more stable models, but will take twice as long to train.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:13: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  del sys.path[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5251 samples, validate on 584 samples\n",
      "Epoch 1/10\n",
      "learning rate 0.01\n",
      "5251/5251 [==============================] - 4s 682us/step - loss: 1.6248 - accuracy: 0.4346 - val_loss: 0.7306 - val_accuracy: 0.3253\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.73058, saving model to model.h5\n",
      "Epoch 2/10\n",
      "learning rate 0.01\n",
      "5251/5251 [==============================] - 2s 458us/step - loss: 1.1544 - accuracy: 0.4991 - val_loss: 0.6883 - val_accuracy: 0.6747\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.73058 to 0.68832, saving model to model.h5\n",
      "Epoch 3/10\n",
      "learning rate 0.01\n",
      "5251/5251 [==============================] - 2s 464us/step - loss: 1.0174 - accuracy: 0.5374 - val_loss: 0.6524 - val_accuracy: 0.6747\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.68832 to 0.65240, saving model to model.h5\n",
      "Epoch 4/10\n",
      "learning rate 0.01\n",
      "5251/5251 [==============================] - 2s 469us/step - loss: 0.8946 - accuracy: 0.5761 - val_loss: 0.6409 - val_accuracy: 0.6747\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.65240 to 0.64091, saving model to model.h5\n",
      "Epoch 5/10\n",
      "learning rate 0.01\n",
      "5251/5251 [==============================] - 2s 460us/step - loss: 0.8464 - accuracy: 0.5896 - val_loss: 0.6457 - val_accuracy: 0.6798\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.64091\n",
      "Epoch 6/10\n",
      "learning rate 0.01\n",
      "5251/5251 [==============================] - 2s 443us/step - loss: 0.7836 - accuracy: 0.6153 - val_loss: 0.6251 - val_accuracy: 0.7038\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.64091 to 0.62509, saving model to model.h5\n",
      "Epoch 7/10\n",
      "learning rate 0.01\n",
      "5251/5251 [==============================] - 2s 442us/step - loss: 0.7472 - accuracy: 0.6245 - val_loss: 0.5722 - val_accuracy: 0.7226\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.62509 to 0.57217, saving model to model.h5\n",
      "Epoch 8/10\n",
      "learning rate 0.01\n",
      "5251/5251 [==============================] - 2s 447us/step - loss: 0.7352 - accuracy: 0.6328 - val_loss: 0.5283 - val_accuracy: 0.7346\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.57217 to 0.52834, saving model to model.h5\n",
      "Epoch 9/10\n",
      "learning rate 0.01\n",
      "5251/5251 [==============================] - 3s 488us/step - loss: 0.7124 - accuracy: 0.6448 - val_loss: 0.5037 - val_accuracy: 0.7466\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.52834 to 0.50368, saving model to model.h5\n",
      "Epoch 10/10\n",
      "learning rate 0.01\n",
      "5251/5251 [==============================] - 3s 485us/step - loss: 0.6793 - accuracy: 0.6509 - val_loss: 0.4901 - val_accuracy: 0.7466\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.50368 to 0.49007, saving model to model.h5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7fe9f03b8d68>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# your code here\n",
    "\n",
    "batch_size=64\n",
    "nb_epoch=10\n",
    "\n",
    "# save our model at the end\n",
    "model_checkpoint = ModelCheckpoint('model.h5', verbose=1, monitor='val_loss', save_best_only=True)\n",
    "\n",
    "# create a learning rate callback\n",
    "learning_rate = LearningRateScheduler(step_decay)\n",
    "\n",
    "model.fit(X_train, y_train, batch_size=batch_size, nb_epoch=nb_epoch, validation_split=0.1,\n",
    "          callbacks=[model_checkpoint,learning_rate], shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the model\n",
    "Output the ROC value.  We will use the ROC to evaluate the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC: 0.812\n"
     ]
    }
   ],
   "source": [
    "# your code here\n",
    "\n",
    "Y_pred = model.predict(X_test, batch_size = 32)\n",
    "roc = roc_auc_score(y_test, Y_pred)\n",
    "print(\"ROC:\", round(roc, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modify the model to remove the dropouts\n",
    "\n",
    "Create the model without the dropouts.  Train, and evalutate this new model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "\n",
    "def create_model(img_channels, img_rows, img_cols):\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Conv2D(16, kernel_size = 3, padding='same', input_shape=(img_channels, img_rows, img_cols)))\n",
    "    model.add(Activation('relu'))\n",
    "\n",
    "    model.add(Conv2D(16, kernel_size = 5, padding=\"same\"))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    \n",
    "    model.add(MaxPooling2D(pool_size = 2, data_format='channels_first'))\n",
    "\n",
    "    model.add(Conv2D(16, kernel_size = 3, padding=\"same\"))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "\n",
    "    model.add(Conv2D(64, kernel_size = 5, padding=\"same\"))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "\n",
    "    model.add(Conv2D(64, kernel_size = 3, padding=\"same\"))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    \n",
    "    model.add(MaxPooling2D(pool_size = 2, data_format='channels_first'))\n",
    "\n",
    "    model.add(Conv2D(128, kernel_size = 3, padding=\"same\"))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "\n",
    "    model.add(MaxPooling2D(pool_size = 2, data_format='channels_first'))\n",
    "    \n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, kernel_initializer=\"he_normal\"))\n",
    "    model.add(Activation('relu'))\n",
    "\n",
    "#     model.add(Dropout(0.5)) # we could also set the rate to zero but it incurs additional processing for no reason\n",
    "    model.add(Dense(32, kernel_initializer=\"he_normal\"))\n",
    "    model.add(Activation('relu'))\n",
    "\n",
    "#     model.add(Dropout(0.5)) \n",
    "    model.add(Dense(1))\n",
    "    model.add(Activation('sigmoid'))\n",
    "    \n",
    "    # learning rate optimizer\n",
    "    optimizer = Adadelta(lr=0)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare\n",
    "Did the ROC increase?  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modify the model\n",
    "Add back in the dropouts, and also add in a new convolution layer.\n",
    "\n",
    "After this code:\n",
    "```\n",
    "    model.add(Conv2D(128, kernel_size = 3, padding=\"same\"))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "\n",
    "```\n",
    "\n",
    "add the following code:\n",
    "```\n",
    "    model.add(Conv2D(128, kernel_size = 5, padding=\"same\"))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "```\n",
    "\n",
    "Recreate, retrain, and evaluate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:6: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5251 samples, validate on 584 samples\n",
      "Epoch 1/10\n",
      "learning rate 0.01\n",
      "5251/5251 [==============================] - 4s 677us/step - loss: 0.7926 - accuracy: 0.6972 - val_loss: 0.6647 - val_accuracy: 0.6747\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.49007\n",
      "Epoch 2/10\n",
      "learning rate 0.01\n",
      "5251/5251 [==============================] - 3s 483us/step - loss: 0.5293 - accuracy: 0.7265 - val_loss: 0.6945 - val_accuracy: 0.4366\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.49007\n",
      "Epoch 3/10\n",
      "learning rate 0.01\n",
      "5251/5251 [==============================] - 2s 471us/step - loss: 0.4892 - accuracy: 0.7587 - val_loss: 0.7335 - val_accuracy: 0.3288\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.49007\n",
      "Epoch 4/10\n",
      "learning rate 0.01\n",
      "5251/5251 [==============================] - 2s 464us/step - loss: 0.4634 - accuracy: 0.7698 - val_loss: 0.8187 - val_accuracy: 0.3390\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.49007\n",
      "Epoch 5/10\n",
      "learning rate 0.01\n",
      "5251/5251 [==============================] - 3s 482us/step - loss: 0.4464 - accuracy: 0.7842 - val_loss: 0.7994 - val_accuracy: 0.4092\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.49007\n",
      "Epoch 6/10\n",
      "learning rate 0.01\n",
      "5251/5251 [==============================] - 2s 467us/step - loss: 0.4291 - accuracy: 0.7957 - val_loss: 0.6978 - val_accuracy: 0.5479\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.49007\n",
      "Epoch 7/10\n",
      "learning rate 0.01\n",
      "5251/5251 [==============================] - 2s 462us/step - loss: 0.4157 - accuracy: 0.8031 - val_loss: 0.5332 - val_accuracy: 0.7277\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.49007\n",
      "Epoch 8/10\n",
      "learning rate 0.01\n",
      "5251/5251 [==============================] - 2s 464us/step - loss: 0.4015 - accuracy: 0.8130 - val_loss: 0.4265 - val_accuracy: 0.8253\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.49007 to 0.42647, saving model to model.h5\n",
      "Epoch 9/10\n",
      "learning rate 0.01\n",
      "5251/5251 [==============================] - 2s 470us/step - loss: 0.3927 - accuracy: 0.8189 - val_loss: 0.3922 - val_accuracy: 0.8408\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.42647 to 0.39224, saving model to model.h5\n",
      "Epoch 10/10\n",
      "learning rate 0.01\n",
      "5251/5251 [==============================] - 2s 439us/step - loss: 0.3811 - accuracy: 0.8278 - val_loss: 0.3752 - val_accuracy: 0.8510\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.39224 to 0.37521, saving model to model.h5\n",
      "ROC: 0.886\n"
     ]
    }
   ],
   "source": [
    "# your code here\n",
    "\n",
    "model = create_model(img_channels, img_rows, img_cols)\n",
    "\n",
    "model.fit(X_train, y_train, batch_size=batch_size, nb_epoch=nb_epoch, validation_split=0.1,\n",
    "          callbacks=[model_checkpoint,learning_rate], shuffle=True)\n",
    "\n",
    "Y_pred = model.predict(X_test, batch_size = 32)\n",
    "roc = roc_auc_score(y_test, Y_pred)\n",
    "print(\"ROC:\", round(roc, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Did this new layer improve the model over the original (first) model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your answer here\n",
    "\n",
    "The last model is likely to have improved the ROC over the original model, and possibly rivaled the overfit model without the Dropouts.  With a very low number of epochs, it's possible that the model fluctuated enough that this wasn't the case when you ran the notebook, but running the final model multiple times should produce a better ROC than the first model.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
