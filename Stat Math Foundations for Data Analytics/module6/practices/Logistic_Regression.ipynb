{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:#2e8dc9; font-size:24px; font-weight:700\"> Logistic Regression</span>\n",
    "\n",
    "In this practice, we will use the same data sets we have used in \n",
    "[Linear Discriminant Analysis practice notebook](Linear_Discriminant_Analysis.ipynb) \n",
    "to demonstrate the concept of linear separability. \n",
    "Take a look at that practice first if you haven't done so yet. \n",
    "\n",
    "We will start with the first data set that has two linearly separable classes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "points_data1 <- read.csv(\"/dsa/data/all_datasets/toydata/data1.csv\",header=TRUE)\n",
    "\n",
    "# For logistic regression, we need to change the class labels from -1 and 1 to 0 and 1. \n",
    "points_data1$class[points_data1$class == -1] <- 0\n",
    "str(points_data1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the data\n",
    "library(ggplot2)\n",
    "plot1 <- ggplot(points_data1, aes(X, Y)) + geom_point(aes(colour = factor(class),shape = factor(class))) + \n",
    "    theme(legend.position = \"none\")\n",
    "plot1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The classes labeled as \"0\" and \"1\" are *linearly separable*; \n",
    "we can draw a linear decision boundary to separate them. \n",
    "Let's apply logistic regression (LR) to predict the class of points. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glmfit1 = glm(class ~ X + Y, data=points_data1, family=binomial)\n",
    "\n",
    "summary(glmfit1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may get a warning about algorithm not converging and probabilities being 0 or 1. \n",
    "Interestingly, if we have a perfectly separable classes in a data set, \n",
    "LR throws this warning because there are infinitely many decision lines that \n",
    "can be drawn between the classes for this data set; \n",
    "LR does not converge to an optimal solution, because optimal solutions are infinitely many. \n",
    "Still, it finds a decision boundary. \n",
    "\n",
    "Let's draw the decision boundary of the LR model on the data. \n",
    "To do that, we'll need to figure out the slope and the intercept of the decision boundary line from the model's coefficients. \n",
    "Lets see how to calculate slope and intercept for this decision boundary.\n",
    "\n",
    "The hypothesis for logistic regression takes the form of:\n",
    "\n",
    "$$h_\\theta = g(z)$$\n",
    "\n",
    "where, $g(z)$ is the sigmoid function and where $z$ is of the form:\n",
    "\n",
    "$$z = \\theta_0 + \\theta_1x_1 + \\theta_2x_2$$\n",
    "\n",
    "Given we are classifying between 0 and 1, $y=1$ when $h_\\theta \\ge 0.5$ which given the sigmoid function is true when:\n",
    "\n",
    "$$ \\theta_0 + \\theta_1x_1 + \\theta_2x_2  \\ge 0$$\n",
    "\n",
    "the above is the decision boundary and can be rearranged as:\n",
    "\n",
    "\n",
    "$$x_2 \\ge \\frac{-\\theta_0}{\\theta_2} + \\frac{-\\theta_1}{\\theta_2}x_1$$\n",
    "\n",
    "This is an equation in the form of $y=mx+b$ and you can see then why $m$ and $b$ \n",
    "are calculated the way they are in the below piece of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glm_slope1 <- coef(glmfit1)[2]/(-coef(glmfit1)[3])\n",
    "glm_intercept1 <- coef(glmfit1)[1]/(-coef(glmfit1)[3])\n",
    "\n",
    "plot1 + geom_abline(slope=glm_slope1, intercept=glm_intercept1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can separate two classes; they are *linearly separable*. \n",
    "Logistic Regression is also a *linear classifier* like LDA; \n",
    "it finds a decision line in two dimensions, a decision plane in three dimensions, \n",
    "and a decision hyperplane for dimensions higher than three. \n",
    "\n",
    "Now, let's compute a confusion table similar to what we have done in LDA practice. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the model on the same data that it was trained with and get the probabilities for each sample. \n",
    "glm1.probs <- predict(glmfit1, type=\"response\")\n",
    "# create an array to hold predictions and assign all zeros initially.\n",
    "glm1.pred = rep(0,length(glm1.probs))\n",
    "# based on model's probablities for each sample, assign the class label.\n",
    "glm1.pred[glm1.probs>0.5] <- 1\n",
    "\n",
    "# Create a confusion table.\n",
    "conftable1 <- table(glm1.pred, points_data1$class)\n",
    "conftable1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that there is no confusion between classes; accuracy is 100% (occasionally, \n",
    "you can have a few misclassified points; \n",
    "that is because a random new data set will be created every time you run the code, \n",
    "and a few points may end up too close to the other class)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's apply LR to the second data set where the classes can't be separated without making some errors. \n",
    "Here, the samples of different classes will be very closely located so that you can't \n",
    "find a linear separation without misclassifying some of them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "points_data2 <- read.csv(\"/dsa/data/all_datasets/toydata/data2.csv\",header=TRUE)\n",
    "\n",
    "# For logistic regression, we need to change the class labels from -1 and 1 to 0 and 1. \n",
    "points_data2$class[points_data2$class == -1] <- 0\n",
    "# Visualize the data\n",
    "plot2 <- ggplot(points_data2, aes(X, Y)) + geom_point(aes(colour=factor(class),shape=factor(class))) + theme(legend.position=\"none\")\n",
    "plot2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above plot, you can see that there is an overlap between classes. \n",
    "This means that some of the samples of a class will be misclassified as the other class; \n",
    "these samples will be on the wrong side of the decision boundary.\n",
    "Let's see that. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glmfit2 = glm(class ~ X + Y, data = points_data2, family = binomial)\n",
    "summary(glmfit2)\n",
    "glm_slope2 <- coef(glmfit2)[2] / (-coef(glmfit2)[3])\n",
    "glm_intercept2 <- coef(glmfit2)[1] / (-coef(glmfit2)[3]) \n",
    "\n",
    "plot2 + geom_abline(slope = glm_slope2, intercept = glm_intercept2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The LR model did not throw a warning; it converged to an optimal solution. \n",
    "The classifier does a good job, but not without mistakes. \n",
    "Let's compute confusion table and the accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the model on the same data that it was trained with and get the probabilities for each sample. \n",
    "glm2.probs <- predict(glmfit2, type = \"response\")\n",
    "\n",
    "# Create an array to hold predictions and assign all zeros initially.\n",
    "glm2.pred = rep(0, length(glm2.probs))\n",
    "\n",
    "# Based on model's probablities for each sample, assign the class label.\n",
    "glm2.pred[glm2.probs > 0.5] <- 1\n",
    "\n",
    "# Create a confusion table.\n",
    "conftable2 <- table(glm2.pred, points_data2$class)\n",
    "conftable2\n",
    "print(paste(\"accuracy = \", sum(diag(conftable2)) / length(glm2.pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will apply the same to the third data set where classes are not linearly separable. \n",
    "**It's your turn:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "points_data3 <- read.csv(\"/dsa/data/all_datasets/toydata/data3.csv\",header=TRUE)\n",
    "\n",
    "# For logistic regression, we need to change the class labels from -1 and 1 to 0 and 1. \n",
    "<what goes in here>\n",
    "# Visualize the data\n",
    "plot3 <- ggplot(points_data3, aes(X, Y)) + geom_point(aes(colour=factor(class),shape=factor(class))) + theme(legend.position=\"none\")\n",
    "plot3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# find model and draw decision boundary\n",
    "glmfit3 = glm(<what goes in here>)\n",
    "summary(glmfit3)\n",
    "\n",
    "glm_slope3 <- coef(glmfit3)[2]/(-coef(glmfit3)[3])\n",
    "glm_intercept3 <- coef(glmfit3)[1]/(-coef(glmfit3)[3]) \n",
    "\n",
    "plot3 + geom_abline(slope=glm_slope3, intercept=glm_intercept3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Run the model on the same data that it was trained with and get the probabilities for each sample. \n",
    "glm3.probs <- <what goes in here>\n",
    "# create an array to hold predictions and assign all zeros initially.\n",
    "glm3.pred = <what goes in here>\n",
    "# based on model's probablities for each sample, assign the class label.\n",
    "glm3.pred[<what goes in here>] <- 1\n",
    "# Create a confusion table.\n",
    "conftable3 <- <what goes in here>\n",
    "conftable3\n",
    "# Compute accuracy\n",
    "print (paste(\"accuracy = \",<what goes in here>))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that LR can not classify this data set successfully; \n",
    "there are many misclassifications (classes are confused for each other). \n",
    "These classes are *not linearly separable*. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, apply the same to the \"XOR pattern\" data set where we have two classes \n",
    "that are linearly nonseparable even though their samples seem to be nicely separated in the plot. \n",
    "\n",
    "**Again, it's your turn.** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "points_data4 <- read.csv(\"/dsa/data/all_datasets/toydata/data4.csv\",header=TRUE)\n",
    "\n",
    "# For logistic regression, we need to change the class labels from -1 and 1 to 0 and 1. \n",
    "<what goes in here>\n",
    "# Visualize the data\n",
    "plot4 <- ggplot(points_data4, aes(X, Y)) + geom_point(aes(colour=factor(class),shape=factor(class))) + theme(legend.position=\"none\")\n",
    "plot4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# find model and draw decision boundary\n",
    "<what goes in here>\n",
    "\n",
    "plot4 + geom_abline(slope=glm_slope4, intercept=glm_intercept4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Run the model on the same data that it was trained with and get the probabilities for each sample. \n",
    "<what goes in here>\n",
    "# create an array to hold predictions and assign all zeros initially.\n",
    "<what goes in here>\n",
    "# based on model's probablities for each sample, assign the class label.\n",
    "<what goes in here>\n",
    "# Create a confusion table.\n",
    "<what goes in here>\n",
    "conftable4\n",
    "# Compute accuracy\n",
    "print (paste(\"accuracy = \",<what goes in here>))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, just like in LDA practice, this is the worst case scenario; \n",
    "the classifier does not do any better than a \"coin toss\" (50% accuracy). \n",
    "Linear models can not deal with this data set. \n",
    "We'll need *nonlinear* models to classify this data set. \n",
    "\n",
    "# Save your notebook!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional Practice\n",
    "\n",
    "It seems like a lot of repetitions of the same thing for different data sets.\n",
    "Could you convert your code above to a function and just call it here for data set *data4.csv* ? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Write your optional function here\n",
    "# ----------------------------------\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Write your optional function here\n",
    "# ----------------------------------\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save your notebook"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
