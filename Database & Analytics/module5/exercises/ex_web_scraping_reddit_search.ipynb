{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Acquisition: Reddit Scraping\n",
    "\n",
    "In this exercise, we will search a query (e.g., \"data science\") on the old Reddit interface (https://www.old.reddit.com/). We will then grab the url (e.g., https://old.reddit.com/search?q=data+science) of the search page and scrap the returned posts. The reason for using the old Reddit interface is that the html tags are user-friendly. We will focus on extracting title, author, author's profile, subreddit, tag, timestamp, number of votes, and number of comments. \n",
    "<img src=\"../images/reddit_search.png\" />\n",
    "\n",
    "\n",
    "\n",
    "* You are free to use your own query string. \n",
    "* On the search page, a set of subreddits are shown. Ignore these subreddits and focus on extracting Reddit posts. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Activity 1:** Fetch the page and create a soup object using Beautiful soup library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code for activity 1 goes here..\n",
    "#---------------------------------------\n",
    "\n",
    "#import the library to query a website\n",
    "import requests\n",
    "# import Beautiful soup library to access \n",
    "# functions to parse the data returned from the website\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "#import pandas to convert list to data frame\n",
    "import pandas as pd\n",
    "#imprt numpy\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "headers = {'User-Agent': 'MyAPP/1.0'}  \n",
    "# this will make sure our query is comming from a browser and it's not a bot\n",
    "\n",
    "\n",
    "# specify the url\n",
    "url = \"https://old.reddit.com/search?q=data+science\"\n",
    "\n",
    "# Open website URL and return the html to the variable 'response'\n",
    "response = requests.get(url, headers=headers)\n",
    "\n",
    "# Parse the html in the 'response' variable, and store it in Beautiful Soup format\n",
    "soup = BeautifulSoup(response.text, \"html\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "attrs = {'class': 'search-result'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Activity 2:** Extract the titles and URLs of the retrieved posts from the soup and print them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Who are your data science heroes?',\n",
       " 'Do you use OOP in your daily data science work?',\n",
       " 'College Professor in Data Science Course Just Said That Functional Programming Is Better Than OOP, Does He Have a Point?',\n",
       " 'Ethics of a data science project I am undertaking',\n",
       " 'The Key Word in Data Science is Science, not Data',\n",
       " \"I built an interactive map to help people self-teaching Data Science online. It's like a skill tree for Data Science!\",\n",
       " 'What data science skills do you see as in-demand given evolution of data science field in last few years?',\n",
       " 'So I, a data science noob, ran sentiment analysis on as much BTS MVs on r/kpop I could find...',\n",
       " 'How I use Data Science to Trade Options Around Earnings',\n",
       " 'Why do people look down on data science work and “computer” work in general?',\n",
       " 'Data Science for the Good of Society: are there realistic employment options?',\n",
       " 'I am interested in creating a group of new comers and intermediate Data science and ML practitioners just to help each other and collaborate for various projects and discussion.',\n",
       " 'MIT to Host First Citizen Data Science Summit on September 20 | Register For Free',\n",
       " 'My Notion layout to manage full-time Physiology and CS studies, part-time data science job, and volunteering',\n",
       " \"Canoo Poaches Lockheed Martin's Manager of Data Systems Architecture, Data Management, Business Intelligence + Data Science (Analytics)\",\n",
       " 'Feeling completely lost (Coursera Data Science with R)',\n",
       " 'Is Data Science useful in GNC teams (or other engineering teams too)? Is there a way to combine DS and GNC?',\n",
       " \"Going back to school to get my masters in the next few years. I'd like to work as a data scientist eventually. Anyone know if it'd be better to get a degree in data science versus statistics?\",\n",
       " 'Turning my Data Science Interview Guide into a full free YouTube Series.',\n",
       " 'How I use Data Science to Trade Options Around Earnings',\n",
       " 'I want to have enough karma to post on data science forums!!',\n",
       " 'Data science freelancers / consultants, what do you actually do?']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titles_html = soup.find_all(\"a\", class_='search-title may-blank')\n",
    "\n",
    "titles = []\n",
    "urls = []\n",
    "\n",
    "for each in titles_html: \n",
    "    titles.append(str(each.get_text()))\n",
    "    \n",
    "titles = titles[3:]\n",
    "    \n",
    "titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://old.reddit.com/r/datascience/comments/pq44jp/who_are_your_data_science_heroes/',\n",
       " 'https://old.reddit.com/r/datascience/comments/pkw92b/do_you_use_oop_in_your_daily_data_science_work/',\n",
       " 'https://old.reddit.com/r/datascience/comments/ppntvz/college_professor_in_data_science_course_just/',\n",
       " 'https://old.reddit.com/r/datascience/comments/prrou1/ethics_of_a_data_science_project_i_am_undertaking/',\n",
       " 'https://old.reddit.com/r/datascience/comments/p7hpd9/the_key_word_in_data_science_is_science_not_data/',\n",
       " 'https://old.reddit.com/r/learndatascience/comments/pjplux/i_built_an_interactive_map_to_help_people/',\n",
       " 'https://old.reddit.com/r/datascience/comments/pj3dls/what_data_science_skills_do_you_see_as_indemand/',\n",
       " 'https://old.reddit.com/r/kpopthoughts/comments/plyyes/so_i_a_data_science_noob_ran_sentiment_analysis/',\n",
       " 'https://old.reddit.com/r/wallstreetbets/comments/psyjv5/how_i_use_data_science_to_trade_options_around/',\n",
       " 'https://old.reddit.com/r/datascience/comments/pmb7a3/why_do_people_look_down_on_data_science_work_and/',\n",
       " 'https://old.reddit.com/r/datascience/comments/p5mzc3/data_science_for_the_good_of_society_are_there/',\n",
       " 'https://old.reddit.com/r/datascience/comments/oy2vfu/i_am_interested_in_creating_a_group_of_new_comers/',\n",
       " 'https://old.reddit.com/r/datascience/comments/pq0wpu/mit_to_host_first_citizen_data_science_summit_on/',\n",
       " 'https://old.reddit.com/r/Notion/comments/ppqeul/my_notion_layout_to_manage_fulltime_physiology/',\n",
       " 'https://old.reddit.com/r/canoo/comments/psobf0/canoo_poaches_lockheed_martins_manager_of_data/',\n",
       " 'https://old.reddit.com/r/rstats/comments/psjo37/feeling_completely_lost_coursera_data_science/',\n",
       " 'https://old.reddit.com/r/AerospaceEngineering/comments/po2e8z/is_data_science_useful_in_gnc_teams_or_other/',\n",
       " 'https://old.reddit.com/r/analytics/comments/pqxmz5/going_back_to_school_to_get_my_masters_in_the/',\n",
       " 'https://old.reddit.com/r/learnmachinelearning/comments/pr7ss4/turning_my_data_science_interview_guide_into_a/',\n",
       " 'https://old.reddit.com/r/smallstreetbets/comments/prg7c8/how_i_use_data_science_to_trade_options_around/',\n",
       " 'https://old.reddit.com/r/FreeKarma4U/comments/pqh6vk/i_want_to_have_enough_karma_to_post_on_data/',\n",
       " 'https://old.reddit.com/r/datascience/comments/prhj81/data_science_freelancers_consultants_what_do_you/']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for each in titles_html: \n",
    "    urls.append(str(each.get('href')))\n",
    "    \n",
    "urls = urls[3:]\n",
    "urls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Activity 3:** Extract the author ids and their profile links from the retrieved posts and print them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['GravityAI',\n",
       " 'rightheart',\n",
       " 'Illustrious_Ice_5022',\n",
       " 'productive_guy123',\n",
       " 'yoi12321',\n",
       " 'InstinctiveDoubt',\n",
       " 'svyas',\n",
       " 'palebabbu',\n",
       " 'nema31lebowski',\n",
       " 'ogretronz',\n",
       " 'saindoja',\n",
       " 'yaakarsh1011',\n",
       " 'saik2363',\n",
       " 'VictorChen1',\n",
       " 'MisterInvicta',\n",
       " 'hyperxenophiliac',\n",
       " 'TheLSales',\n",
       " 'fu11m3ta1',\n",
       " 'SnooPaintings5866',\n",
       " 'kribz666',\n",
       " 'bingingwithdata',\n",
       " 'Kokubo-ubo']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "authors_html = soup.find_all(\"span\", class_='search-author')\n",
    "\n",
    "authors = []\n",
    "\n",
    "for each in authors_html: \n",
    "    authors.append(str(each.get_text()))\n",
    "    \n",
    "authors = [e[3:] for e in authors]\n",
    "\n",
    "authors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://old.reddit.com/user/GravityAI',\n",
       " 'https://old.reddit.com/user/rightheart',\n",
       " 'https://old.reddit.com/user/Illustrious_Ice_5022',\n",
       " 'https://old.reddit.com/user/productive_guy123',\n",
       " 'https://old.reddit.com/user/yoi12321',\n",
       " 'https://old.reddit.com/user/InstinctiveDoubt',\n",
       " 'https://old.reddit.com/user/svyas',\n",
       " 'https://old.reddit.com/user/palebabbu',\n",
       " 'https://old.reddit.com/user/nema31lebowski',\n",
       " 'https://old.reddit.com/user/ogretronz',\n",
       " 'https://old.reddit.com/user/saindoja',\n",
       " 'https://old.reddit.com/user/yaakarsh1011',\n",
       " 'https://old.reddit.com/user/saik2363',\n",
       " 'https://old.reddit.com/user/VictorChen1',\n",
       " 'https://old.reddit.com/user/MisterInvicta',\n",
       " 'https://old.reddit.com/user/hyperxenophiliac',\n",
       " 'https://old.reddit.com/user/TheLSales',\n",
       " 'https://old.reddit.com/user/fu11m3ta1',\n",
       " 'https://old.reddit.com/user/SnooPaintings5866',\n",
       " 'https://old.reddit.com/user/kribz666',\n",
       " 'https://old.reddit.com/user/bingingwithdata',\n",
       " 'https://old.reddit.com/user/Kokubo-ubo']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "links_html = soup.find_all(class_=re.compile('author may-blank id-.*'))\n",
    "    \n",
    "links = []\n",
    "\n",
    "for each in links_html: \n",
    "    links.append(str(each.get('href')))\n",
    "    \n",
    "links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Activity 4:** Extract the submission time of the retrieved posts and print them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['6 days ago',\n",
       " '14 days ago',\n",
       " '6 days ago',\n",
       " '3 days ago',\n",
       " '1 month ago',\n",
       " '16 days ago',\n",
       " '17 days ago',\n",
       " '12 days ago',\n",
       " '1 day ago',\n",
       " '12 days ago',\n",
       " '1 month ago',\n",
       " '1 month ago',\n",
       " '6 days ago',\n",
       " '6 days ago',\n",
       " '2 days ago',\n",
       " '2 days ago',\n",
       " '9 days ago',\n",
       " '4 days ago',\n",
       " '4 days ago',\n",
       " '3 days ago',\n",
       " '5 days ago',\n",
       " '3 days ago']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "times_html = soup.find_all(\"span\", class_='search-time')\n",
    "\n",
    "times = []\n",
    "\n",
    "for each in times_html: \n",
    "    times.append(str(each.get_text()))\n",
    "    \n",
    "times = times[3:]\n",
    "\n",
    "times = [e[10:] for e in times]\n",
    "\n",
    "times"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Activity 5:** Extract the subreddits of the retrieved posts and print them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['r/datascience',\n",
       " 'r/datascience',\n",
       " 'r/datascience',\n",
       " 'r/datascience',\n",
       " 'r/datascience',\n",
       " 'r/learndatascience',\n",
       " 'r/datascience',\n",
       " 'r/kpopthoughts',\n",
       " 'r/wallstreetbets',\n",
       " 'r/datascience',\n",
       " 'r/datascience',\n",
       " 'r/datascience',\n",
       " 'r/datascience',\n",
       " 'r/Notion',\n",
       " 'r/canoo',\n",
       " 'r/rstats',\n",
       " 'r/AerospaceEngineering',\n",
       " 'r/analytics',\n",
       " 'r/learnmachinelearning',\n",
       " 'r/smallstreetbets',\n",
       " 'r/FreeKarma4U',\n",
       " 'r/datascience']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subreddits_html = soup.find_all(\"a\", class_='search-subreddit-link may-blank')\n",
    "\n",
    "subreddits = []\n",
    "\n",
    "for each in subreddits_html: \n",
    "    subreddits.append(str(each.get_text()))\n",
    "    \n",
    "subreddits = subreddits[3:]\n",
    "    \n",
    "subreddits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Activity 6:** Extract the associated tag(s) of the retrieved posts and print them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Fun/Trivia',\n",
       " 'Discussion',\n",
       " 'Discussion',\n",
       " 'Discussion',\n",
       " 'Discussion',\n",
       " 'Resources',\n",
       " 'Discussion',\n",
       " 'Boy Groups',\n",
       " 'Discussion',\n",
       " 'Discussion',\n",
       " 'Career',\n",
       " 'Networking',\n",
       " 'Networking',\n",
       " 'Showcase',\n",
       " 'New Hires',\n",
       " nan,\n",
       " 'Career',\n",
       " 'Question',\n",
       " 'Tutorial',\n",
       " 'Discussion',\n",
       " nan,\n",
       " 'Career']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tags_html = soup.find_all(\"span\", class_=\"linkflairlabel\")\n",
    "\n",
    "tags = []\n",
    "\n",
    "for each in tags_html: \n",
    "    tags.append(str(each.get_text()))\n",
    "    \n",
    "tags.insert(15, np.nan) # this post has no tag\n",
    "tags.insert(20, np.nan) # this post has no tag\n",
    "\n",
    "tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Activity 7:** Extract the points of the retrieved posts and print them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['194',\n",
       " '210',\n",
       " '110',\n",
       " '73',\n",
       " '1,208',\n",
       " '746',\n",
       " '194',\n",
       " '183',\n",
       " '42',\n",
       " '52',\n",
       " '245',\n",
       " '337',\n",
       " '156',\n",
       " '161',\n",
       " '56',\n",
       " '24',\n",
       " '22',\n",
       " '33',\n",
       " '211',\n",
       " '98',\n",
       " '7',\n",
       " '33']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "points_html = soup.find_all(\"span\", class_='search-score')\n",
    "\n",
    "points = []\n",
    "\n",
    "for each in points_html: \n",
    "    points.append(str(each.get_text()))\n",
    "    \n",
    "points = [e[:-7] for e in points]\n",
    "\n",
    "points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Activity 8:** Extract the num of comments of the retrieved posts and print them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['119',\n",
       " '158',\n",
       " '128',\n",
       " '53',\n",
       " '156',\n",
       " '54',\n",
       " '95',\n",
       " '50',\n",
       " '31',\n",
       " '63',\n",
       " '152',\n",
       " '238',\n",
       " '19',\n",
       " '17',\n",
       " '17',\n",
       " '28',\n",
       " '40',\n",
       " '24',\n",
       " '9',\n",
       " '12',\n",
       " '44',\n",
       " '14']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments_html = soup.find_all(\"a\", class_='search-comments may-blank')\n",
    "\n",
    "comments = []\n",
    "\n",
    "for each in comments_html: \n",
    "    comments.append(str(each.get_text()))\n",
    "    \n",
    "comments = [e[:-9] for e in comments]\n",
    "    \n",
    "comments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Activity 9:** Using the above nine features create a dataframe for the retrieved posts, and print the first 10 entries. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>URL</th>\n",
       "      <th>Author</th>\n",
       "      <th>Profile</th>\n",
       "      <th>Time</th>\n",
       "      <th>Subreddit</th>\n",
       "      <th>Tag</th>\n",
       "      <th>Points</th>\n",
       "      <th>Comments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Who are your data science heroes?</td>\n",
       "      <td>https://old.reddit.com/r/datascience/comments/...</td>\n",
       "      <td>GravityAI</td>\n",
       "      <td>https://old.reddit.com/user/GravityAI</td>\n",
       "      <td>6 days ago</td>\n",
       "      <td>r/datascience</td>\n",
       "      <td>Fun/Trivia</td>\n",
       "      <td>194</td>\n",
       "      <td>119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Do you use OOP in your daily data science work?</td>\n",
       "      <td>https://old.reddit.com/r/datascience/comments/...</td>\n",
       "      <td>rightheart</td>\n",
       "      <td>https://old.reddit.com/user/rightheart</td>\n",
       "      <td>14 days ago</td>\n",
       "      <td>r/datascience</td>\n",
       "      <td>Discussion</td>\n",
       "      <td>210</td>\n",
       "      <td>158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>College Professor in Data Science Course Just ...</td>\n",
       "      <td>https://old.reddit.com/r/datascience/comments/...</td>\n",
       "      <td>Illustrious_Ice_5022</td>\n",
       "      <td>https://old.reddit.com/user/Illustrious_Ice_5022</td>\n",
       "      <td>6 days ago</td>\n",
       "      <td>r/datascience</td>\n",
       "      <td>Discussion</td>\n",
       "      <td>110</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ethics of a data science project I am undertaking</td>\n",
       "      <td>https://old.reddit.com/r/datascience/comments/...</td>\n",
       "      <td>productive_guy123</td>\n",
       "      <td>https://old.reddit.com/user/productive_guy123</td>\n",
       "      <td>3 days ago</td>\n",
       "      <td>r/datascience</td>\n",
       "      <td>Discussion</td>\n",
       "      <td>73</td>\n",
       "      <td>53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The Key Word in Data Science is Science, not Data</td>\n",
       "      <td>https://old.reddit.com/r/datascience/comments/...</td>\n",
       "      <td>yoi12321</td>\n",
       "      <td>https://old.reddit.com/user/yoi12321</td>\n",
       "      <td>1 month ago</td>\n",
       "      <td>r/datascience</td>\n",
       "      <td>Discussion</td>\n",
       "      <td>1,208</td>\n",
       "      <td>156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>I built an interactive map to help people self...</td>\n",
       "      <td>https://old.reddit.com/r/learndatascience/comm...</td>\n",
       "      <td>InstinctiveDoubt</td>\n",
       "      <td>https://old.reddit.com/user/InstinctiveDoubt</td>\n",
       "      <td>16 days ago</td>\n",
       "      <td>r/learndatascience</td>\n",
       "      <td>Resources</td>\n",
       "      <td>746</td>\n",
       "      <td>54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>What data science skills do you see as in-dema...</td>\n",
       "      <td>https://old.reddit.com/r/datascience/comments/...</td>\n",
       "      <td>svyas</td>\n",
       "      <td>https://old.reddit.com/user/svyas</td>\n",
       "      <td>17 days ago</td>\n",
       "      <td>r/datascience</td>\n",
       "      <td>Discussion</td>\n",
       "      <td>194</td>\n",
       "      <td>95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>So I, a data science noob, ran sentiment analy...</td>\n",
       "      <td>https://old.reddit.com/r/kpopthoughts/comments...</td>\n",
       "      <td>palebabbu</td>\n",
       "      <td>https://old.reddit.com/user/palebabbu</td>\n",
       "      <td>12 days ago</td>\n",
       "      <td>r/kpopthoughts</td>\n",
       "      <td>Boy Groups</td>\n",
       "      <td>183</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>How I use Data Science to Trade Options Around...</td>\n",
       "      <td>https://old.reddit.com/r/wallstreetbets/commen...</td>\n",
       "      <td>nema31lebowski</td>\n",
       "      <td>https://old.reddit.com/user/nema31lebowski</td>\n",
       "      <td>1 day ago</td>\n",
       "      <td>r/wallstreetbets</td>\n",
       "      <td>Discussion</td>\n",
       "      <td>42</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Why do people look down on data science work a...</td>\n",
       "      <td>https://old.reddit.com/r/datascience/comments/...</td>\n",
       "      <td>ogretronz</td>\n",
       "      <td>https://old.reddit.com/user/ogretronz</td>\n",
       "      <td>12 days ago</td>\n",
       "      <td>r/datascience</td>\n",
       "      <td>Discussion</td>\n",
       "      <td>52</td>\n",
       "      <td>63</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Title  \\\n",
       "0                  Who are your data science heroes?   \n",
       "1    Do you use OOP in your daily data science work?   \n",
       "2  College Professor in Data Science Course Just ...   \n",
       "3  Ethics of a data science project I am undertaking   \n",
       "4  The Key Word in Data Science is Science, not Data   \n",
       "5  I built an interactive map to help people self...   \n",
       "6  What data science skills do you see as in-dema...   \n",
       "7  So I, a data science noob, ran sentiment analy...   \n",
       "8  How I use Data Science to Trade Options Around...   \n",
       "9  Why do people look down on data science work a...   \n",
       "\n",
       "                                                 URL                Author  \\\n",
       "0  https://old.reddit.com/r/datascience/comments/...             GravityAI   \n",
       "1  https://old.reddit.com/r/datascience/comments/...            rightheart   \n",
       "2  https://old.reddit.com/r/datascience/comments/...  Illustrious_Ice_5022   \n",
       "3  https://old.reddit.com/r/datascience/comments/...     productive_guy123   \n",
       "4  https://old.reddit.com/r/datascience/comments/...              yoi12321   \n",
       "5  https://old.reddit.com/r/learndatascience/comm...      InstinctiveDoubt   \n",
       "6  https://old.reddit.com/r/datascience/comments/...                 svyas   \n",
       "7  https://old.reddit.com/r/kpopthoughts/comments...             palebabbu   \n",
       "8  https://old.reddit.com/r/wallstreetbets/commen...        nema31lebowski   \n",
       "9  https://old.reddit.com/r/datascience/comments/...             ogretronz   \n",
       "\n",
       "                                            Profile         Time  \\\n",
       "0             https://old.reddit.com/user/GravityAI   6 days ago   \n",
       "1            https://old.reddit.com/user/rightheart  14 days ago   \n",
       "2  https://old.reddit.com/user/Illustrious_Ice_5022   6 days ago   \n",
       "3     https://old.reddit.com/user/productive_guy123   3 days ago   \n",
       "4              https://old.reddit.com/user/yoi12321  1 month ago   \n",
       "5      https://old.reddit.com/user/InstinctiveDoubt  16 days ago   \n",
       "6                 https://old.reddit.com/user/svyas  17 days ago   \n",
       "7             https://old.reddit.com/user/palebabbu  12 days ago   \n",
       "8        https://old.reddit.com/user/nema31lebowski    1 day ago   \n",
       "9             https://old.reddit.com/user/ogretronz  12 days ago   \n",
       "\n",
       "            Subreddit         Tag Points Comments  \n",
       "0       r/datascience  Fun/Trivia    194      119  \n",
       "1       r/datascience  Discussion    210      158  \n",
       "2       r/datascience  Discussion    110      128  \n",
       "3       r/datascience  Discussion     73       53  \n",
       "4       r/datascience  Discussion  1,208      156  \n",
       "5  r/learndatascience   Resources    746       54  \n",
       "6       r/datascience  Discussion    194       95  \n",
       "7      r/kpopthoughts  Boy Groups    183       50  \n",
       "8    r/wallstreetbets  Discussion     42       31  \n",
       "9       r/datascience  Discussion     52       63  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reddit_df = df=pd.DataFrame({'Title':titles, \n",
    "                                'URL': urls,\n",
    "                                'Author':authors,\n",
    "                                'Profile':links,\n",
    "                                'Time':times,\n",
    "                                'Subreddit': subreddits,\n",
    "                                'Tag': tags,\n",
    "                                'Points':points,\n",
    "                                'Comments':comments\n",
    "                               })\n",
    "\n",
    "reddit_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Activity 10:** Save the retrieved posts in a json file.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit_df.to_json('reddit.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save your notebook, then `File > Close and Halt`"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
