{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Processing: Stemming and Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Original Dataset: https://www.kaggle.com/uciml/sms-spam-collection-dataset/home*\n",
    "\n",
    "For this exercise, there are 100 sms that have been parsed and categorized as \"Spam\" or \"Ham\". The dataframe also contains the original text message. We have converted the dataframe into a dictionary for this exercise (execute the first two cells).\n",
    "\n",
    "In the given dictionary, there are 100 entries, starting from 0 to 99 as the keys. The value for each of them is two strings, `class` and `text`. `class` contains either \"spam\" or \"ham\", based on the category of the sms, and `text` contains the original text message."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"/dsa/data/DSA-8410/spam.csv\", encoding='latin1')\n",
    "mini_df = df[['v1', 'v2']][:100]\n",
    "mini_df.columns = ['class', 'text']\n",
    "\n",
    "mini_df.to_csv('messages.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('messages.csv')\n",
    "msgs = df.T.to_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 1.** Create a list of strings from this dictionary with the `text` values, and convert all of the strings into lowercase. Print out the first five (5) items from your list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['go until jurong point, crazy.. available only in bugis n great world la e buffet... cine there got amore wat...', 'ok lar... joking wif u oni...', \"free entry in 2 a wkly comp to win fa cup final tkts 21st may 2005. text fa to 87121 to receive entry question(std txt rate)t&c's apply 08452810075over18's\", 'u dun say so early hor... u c already then say...', \"nah i don't think he goes to usf, he lives around here though\"]\n"
     ]
    }
   ],
   "source": [
    "# Your code goes here\n",
    "#---------------------\n",
    "\n",
    "text_list = [val['text'] for key, val in msgs.items() if 'text' in val]\n",
    "\n",
    "text_list = [each_string.lower() for each_string in text_list]\n",
    "\n",
    "print(text_list[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 2.** Use `nltk` packages tokenize functionality on each of the strings in your list. The result should be a list of lists. Print out the first five (5) items from your list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['go', 'until', 'jurong', 'point', ',', 'crazy..', 'available', 'only', 'in', 'bugis', 'n', 'great', 'world', 'la', 'e', 'buffet', '...', 'cine', 'there', 'got', 'amore', 'wat', '...'], ['ok', 'lar', '...', 'joking', 'wif', 'u', 'oni', '...'], ['free', 'entry', 'in', '2', 'a', 'wkly', 'comp', 'to', 'win', 'fa', 'cup', 'final', 'tkts', '21st', 'may', '2005.', 'text', 'fa', 'to', '87121', 'to', 'receive', 'entry', 'question', '(', 'std', 'txt', 'rate', ')', 't', '&', 'c', \"'s\", 'apply', '08452810075over18', \"'s\"], ['u', 'dun', 'say', 'so', 'early', 'hor', '...', 'u', 'c', 'already', 'then', 'say', '...'], ['nah', 'i', 'do', \"n't\", 'think', 'he', 'goes', 'to', 'usf', ',', 'he', 'lives', 'around', 'here', 'though']]\n"
     ]
    }
   ],
   "source": [
    "# Your code goes here\n",
    "#---------------------\n",
    "\n",
    "from nltk import word_tokenize\n",
    "\n",
    "tokens = [word_tokenize(i) for i in text_list]\n",
    "\n",
    "print(tokens[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 3.** Remove the stopwords, punctuations and numbers from your list (list of lists). Punctuations and numbers can be checked by the function `string.punctuation` used after a string. If the result is false, you can remove that particular string from the list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['go', 'jurong', 'point', 'available', 'bugis', 'n', 'great', 'world', 'la', 'e', 'buffet', 'cine', 'got', 'amore', 'wat'], ['ok', 'lar', 'joking', 'wif', 'u', 'oni'], ['free', 'entry', 'wkly', 'comp', 'win', 'fa', 'cup', 'final', 'tkts', 'may', 'text', 'fa', 'receive', 'entry', 'question', 'std', 'txt', 'rate', 'c', 'apply'], ['u', 'dun', 'say', 'early', 'hor', 'u', 'c', 'already', 'say'], ['nah', 'think', 'goes', 'usf', 'lives', 'around', 'though'], ['freemsg', 'hey', 'darling', 'week', 'word', 'back', 'like', 'fun', 'still', 'tb', 'ok', 'xxx', 'std', 'chgs', 'send', 'rcv'], ['even', 'brother', 'like', 'speak', 'treat', 'like', 'aids', 'patent'], ['per', 'request', 'melle', 'oru', 'minnaminunginte', 'nurungu', 'vettam', 'set', 'callertune', 'callers', 'press', 'copy', 'friends', 'callertune'], ['winner', 'valued', 'network', 'customer', 'selected', 'receivea', 'prize', 'reward', 'claim', 'call', 'claim', 'code', 'valid', 'hours'], ['mobile', 'months', 'u', 'r', 'entitled', 'update', 'latest', 'colour', 'mobiles', 'camera', 'free', 'call', 'mobile', 'update', 'co', 'free']]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/dcphw2/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "# Your code goes here\n",
    "#---------------------\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download(\"stopwords\")\n",
    "stop_words = stopwords.words(\"english\")\n",
    "\n",
    "for i in range(len(tokens)):\n",
    "    for lst in tokens[i]:\n",
    "        tokens[i] = [word for word in tokens[i] if word.isalpha()] # seemed like this made more sense than string.punctuation\n",
    "        tokens[i] = [word for word in tokens[i] if word not in stop_words]\n",
    "\n",
    "print(tokens[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 4.** Use `nltk` packages `PorterStemmer` to stem the cleaned-text list that you got as a result of **Task 3**. Use a new variable to store the stemmed-word list, and keep the result from the **Task 3** intact. As we will use the cleaned-text list from **Task 3** in the later tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['go', 'jurong', 'point', 'avail', 'bugi', 'n', 'great', 'world', 'la', 'e', 'buffet', 'cine', 'got', 'amor', 'wat'], ['ok', 'lar', 'joke', 'wif', 'u', 'oni'], ['free', 'entri', 'wkli', 'comp', 'win', 'fa', 'cup', 'final', 'tkt', 'may', 'text', 'fa', 'receiv', 'entri', 'question', 'std', 'txt', 'rate', 'c', 'appli'], ['u', 'dun', 'say', 'earli', 'hor', 'u', 'c', 'alreadi', 'say'], ['nah', 'think', 'goe', 'usf', 'live', 'around', 'though'], ['freemsg', 'hey', 'darl', 'week', 'word', 'back', 'like', 'fun', 'still', 'tb', 'ok', 'xxx', 'std', 'chg', 'send', 'rcv'], ['even', 'brother', 'like', 'speak', 'treat', 'like', 'aid', 'patent'], ['per', 'request', 'mell', 'oru', 'minnaminungint', 'nurungu', 'vettam', 'set', 'callertun', 'caller', 'press', 'copi', 'friend', 'callertun'], ['winner', 'valu', 'network', 'custom', 'select', 'receivea', 'prize', 'reward', 'claim', 'call', 'claim', 'code', 'valid', 'hour'], ['mobil', 'month', 'u', 'r', 'entitl', 'updat', 'latest', 'colour', 'mobil', 'camera', 'free', 'call', 'mobil', 'updat', 'co', 'free']]\n"
     ]
    }
   ],
   "source": [
    "# Your code goes here\n",
    "#---------------------\n",
    "from nltk.stem import PorterStemmer\n",
    "porter = PorterStemmer()\n",
    "\n",
    "stems = []\n",
    "\n",
    "for i in range(len(tokens)):\n",
    "    stems.append([porter.stem(word) for word in tokens[i]])\n",
    "\n",
    "print(stems[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 5.** Use `nltk` packages `WordNetLemmatizer` to find the lemma (or root word) from the cleaned-text list that you got as a result of **Task 3**. Consider all of the words to be a `Verb`. Use a new variable to store the lemmatized-word list, and keep the result from **Task 3** intact. As we will use the cleaned-text list from **Task 3** in the later tasks. We assume every word is a verb to make the problem easier, but we could have applied a `POS` tagger and inferred the POS for that word. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['go', 'jurong', 'point', 'available', 'bugis', 'n', 'great', 'world', 'la', 'e', 'buffet', 'cine', 'get', 'amore', 'wat'], ['ok', 'lar', 'joke', 'wif', 'u', 'oni'], ['free', 'entry', 'wkly', 'comp', 'win', 'fa', 'cup', 'final', 'tkts', 'may', 'text', 'fa', 'receive', 'entry', 'question', 'std', 'txt', 'rate', 'c', 'apply'], ['u', 'dun', 'say', 'early', 'hor', 'u', 'c', 'already', 'say'], ['nah', 'think', 'go', 'usf', 'live', 'around', 'though'], ['freemsg', 'hey', 'darling', 'week', 'word', 'back', 'like', 'fun', 'still', 'tb', 'ok', 'xxx', 'std', 'chgs', 'send', 'rcv'], ['even', 'brother', 'like', 'speak', 'treat', 'like', 'aid', 'patent'], ['per', 'request', 'melle', 'oru', 'minnaminunginte', 'nurungu', 'vettam', 'set', 'callertune', 'callers', 'press', 'copy', 'friends', 'callertune'], ['winner', 'value', 'network', 'customer', 'select', 'receivea', 'prize', 'reward', 'claim', 'call', 'claim', 'code', 'valid', 'hours'], ['mobile', 'months', 'u', 'r', 'entitle', 'update', 'latest', 'colour', 'mobiles', 'camera', 'free', 'call', 'mobile', 'update', 'co', 'free']]\n"
     ]
    }
   ],
   "source": [
    "# Your code goes here\n",
    "#---------------------\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wordnet = WordNetLemmatizer()\n",
    "\n",
    "lemmas = []\n",
    "\n",
    "for i in range(len(tokens)):\n",
    "     lemmas.append([wordnet.lemmatize(word, pos=\"v\") for word in tokens[i]])\n",
    "        \n",
    "print(lemmas[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 6.** For each lemma that we got from **Task 5**, calculate how many times they occur in all of the messages. Sort them in descending order by the number of total occurrences, and print out the top ten (10) words and their number of occurrences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u          17\n",
       "call       14\n",
       "get        11\n",
       "go         11\n",
       "like       10\n",
       "free        9\n",
       "sorry       8\n",
       "ok          8\n",
       "smile       6\n",
       "already     6\n",
       "dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Your code goes here\n",
    "#---------------------\n",
    "\n",
    "lemmas_count = pd.Series(lemmas).explode().value_counts()\n",
    "lemmas_count.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 7.** From the result we got from **Task 6**, remove all of the words with a length of 1 and select the top hundred (100) most frequent terms from it. We will use this list of words in our next task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u            17\n",
       "call         14\n",
       "get          11\n",
       "go           11\n",
       "like         10\n",
       "             ..\n",
       "something     2\n",
       "car           2\n",
       "run           2\n",
       "name          2\n",
       "months        2\n",
       "Length: 100, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Your code goes here\n",
    "#---------------------\n",
    "\n",
    "greater_one = lemmas_count[lemmas_count > 1]\n",
    "\n",
    "greater_one = greater_one.head(100)\n",
    "\n",
    "greater_one"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 8.** For each message (use the lemma-list we created for **Task 5**), calculate the number of times each word from **Task 7** (top-100 words) occurs in that message. \n",
    "Create a **Data-Matrix** using your calculations. Each row should correspond to a message, and each column should correspond to a word from the list we got in **Task 7**. Each cell should correspond to how many times that particular word (from column) occurs for that specific message (from row).\n",
    "\n",
    "You can use Pandas-DataFrame to store your **Data-Matrix**. Print the first five rows of the Data-Matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>u</th>\n",
       "      <th>call</th>\n",
       "      <th>get</th>\n",
       "      <th>go</th>\n",
       "      <th>like</th>\n",
       "      <th>free</th>\n",
       "      <th>sorry</th>\n",
       "      <th>ok</th>\n",
       "      <th>smile</th>\n",
       "      <th>already</th>\n",
       "      <th>...</th>\n",
       "      <th>watch</th>\n",
       "      <th>charge</th>\n",
       "      <th>cut</th>\n",
       "      <th>gram</th>\n",
       "      <th>work</th>\n",
       "      <th>something</th>\n",
       "      <th>car</th>\n",
       "      <th>run</th>\n",
       "      <th>name</th>\n",
       "      <th>months</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   u  call  get  go  like  free  sorry  ok  smile  already  ...  watch  \\\n",
       "0  0     0    1   1     0     0      0   0      0        0  ...      0   \n",
       "1  1     0    0   0     0     0      0   1      0        0  ...      0   \n",
       "2  0     0    0   0     0     1      0   0      0        0  ...      0   \n",
       "3  2     0    0   0     0     0      0   0      0        1  ...      0   \n",
       "4  0     0    0   1     0     0      0   0      0        0  ...      0   \n",
       "\n",
       "   charge  cut  gram  work  something  car  run  name  months  \n",
       "0       0    0     0     0          0    0    0     0       0  \n",
       "1       0    0     0     0          0    0    0     0       0  \n",
       "2       0    0     0     0          0    0    0     0       0  \n",
       "3       0    0     0     0          0    0    0     0       0  \n",
       "4       0    0     0     0          0    0    0     0       0  \n",
       "\n",
       "[5 rows x 100 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Your code goes here\n",
    "#---------------------\n",
    "\n",
    "word_counts = []\n",
    "\n",
    "for i in range(len(lemmas)):\n",
    "    for j in range(len(greater_one.index)):\n",
    "        word_counts.append(lemmas[i].count(greater_one.index[j])) # created one large list\n",
    "        \n",
    "word_counts = [word_counts[x:x + 100] for x in range(0, len(word_counts), 100)] # broke list down into each 100 values\n",
    "\n",
    "df = pd.DataFrame(word_counts, columns = greater_one.index)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
