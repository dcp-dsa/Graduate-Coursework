{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise: Building and Loading Text Search in Python Whoosh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "<a id='task' ></a>\n",
    "\n",
    "## Task at hand\n",
    "\n",
    "\n",
    "For this exercise, we are going to walk through the process of creating full text search capability within Python for integration into other analytical processes.\n",
    "\n",
    "You previously worked with the _`book`_ data. In this exercise, we will work with some wiki data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "<a id='build_it' ></a>\n",
    "\n",
    "## Buiding our Whoosh Schema\n",
    "\n",
    "Recall, the `book/` folder is composed of a collection of text files, each its own book chapter.\n",
    "\n",
    "In whoosh, we structure the retrieval system by defining a storage schema.\n",
    "\n",
    "From the lab with the text files:\n",
    "```\n",
    "from whoosh.fields import Schema, TEXT, KEYWORD, ID, STORED\n",
    "from whoosh.analysis import StemmingAnalyzer\n",
    "\n",
    "schema = Schema(filename=ID(stored=True),\n",
    "                content=TEXT(analyzer=StemmingAnalyzer())\n",
    "                )\n",
    "```\n",
    "\n",
    "This tells us we are defining records to have a `(filename, content)`\n",
    "\n",
    "For this exercise, we will be using a few Wikipedia pages for our data source.\n",
    "\n",
    "### 1) For this exercise, you should look at a few of these web pages:\n",
    "\n",
    "  * https://en.wikipedia.org/wiki/Nyctimantis\n",
    "  * https://en.wikipedia.org/wiki/Osteocephalus\n",
    "  * https://en.wikipedia.org/wiki/Osteopilus\n",
    "  \n",
    "Specifically, inspect the HTML source and the \n",
    "```HTML\n",
    "<table class=\"infobox biota\" ... </table>\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"../images/table_inspect.png\" height=400 width=600 />\n",
    "\n",
    "\n",
    "\n",
    "**Task: You need to extend the above schema definition to collect this frog table data when available.**\n",
    "\n",
    "* Content will be the all visible text on the html page\n",
    "* Table information such as kingdom, phylum, class, order, family, subfamily, genus should be searchable "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change this to a code cell and run if you have trouble with a locked writer\n",
    "from whoosh import writing\n",
    "writer.commit(mergetype=writing.CLEAR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from whoosh.fields import Schema, TEXT, KEYWORD, ID, STORED\n",
    "from whoosh.analysis import StemmingAnalyzer\n",
    "\n",
    "schema = Schema(filename=ID(stored=True),\n",
    "                content=TEXT(analyzer=StemmingAnalyzer()),\n",
    "                # Extend the schema definition to capture relevant table data\n",
    "                kingdom=KEYWORD(stored=True, analyzer=StemmingAnalyzer(), scorable=True),\n",
    "                phylum=KEYWORD(stored=True, analyzer=StemmingAnalyzer(), scorable=True),\n",
    "                an_class=KEYWORD(stored=True, analyzer=StemmingAnalyzer(), scorable=True),\n",
    "                an_order=KEYWORD(stored=True, analyzer=StemmingAnalyzer(), scorable=True),\n",
    "                family=KEYWORD(stored=True, analyzer=StemmingAnalyzer(), scorable=True),\n",
    "                subfamily=KEYWORD(stored=True, analyzer=StemmingAnalyzer(), scorable=True),\n",
    "                genus=KEYWORD(stored=True, analyzer=StemmingAnalyzer(), scorable=True)\n",
    "               )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "--- \n",
    "<a id='load_it' ></a>\n",
    "\n",
    "## Loading Data\n",
    "\n",
    "For this exercise, we have created a small folder of a few Wikipedia pages under the `en.wikipedia.org/wiki` folder in the common datasets folder:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acris.html\t     Hylidae.html\t   Plectrohyla.html\r\n",
      "Anotheca.html\t     Hylinae.html\t   Pseudacris.html\r\n",
      "Aparasphenodon.html  Hyloscirtus.html\t   Pseudis.html\r\n",
      "Aplastodiscus.html   Hypsiboas.html\t   Ptychohyla.html\r\n",
      "Argenteohyla.html    Isthmohyla.html\t   Scarthyla.html\r\n",
      "Bokermannohyla.html  Itapotihyla.html\t   Scinax.html\r\n",
      "Bromeliohyla.html    Lysapsus.html\t   Smilisca.html\r\n",
      "Charadrahyla.html    Megastomatohyla.html  Sphaenorhynchus.html\r\n",
      "Corythomantis.html   Myersiohyla.html\t   Tepuihyla.html\r\n",
      "Dendropsophus.html   Nyctimantis.html\t   Tlalocohyla.html\r\n",
      "Duellmanohyla.html   Osteocephalus.html    Trachycephalus.html\r\n",
      "Ecnomiohyla.html     Osteopilus.html\t   Triprion.html\r\n",
      "Exerodonta.html      Phyllodytes.html\t   Xenohyla.html\r\n",
      "Hyla.html\t     Phytotriades.html\r\n"
     ]
    }
   ],
   "source": [
    "! ls /dsa/data/all_datasets/en.wikipedia.org/wiki"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "You will create the _whoosh_ index files in the `modules/module6/exercises/wiki_index` folder then ingest the files.\n",
    "\n",
    "To load the data, write a python script that follow the basic crawling behavior\n",
    "\n",
    " 1. For each file/folder in the specified starting folder:\n",
    " 1. If it is a folder, recurse into folder and process contents\n",
    " 1. If it is a file, read contents and load into indexer.\n",
    " \n",
    "## Follow the lab for Python IR with whoosh to complete this exercise.\n",
    "\n",
    "### 2) Create / Initialize the whoosh index and get the `writer` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, os.path\n",
    "from whoosh import index\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "headers = {'User-Agent': 'MyAPP/1.0'}\n",
    "\n",
    "attrs = {'class': 'search-result'}\n",
    "\n",
    "# Step 2 below this comment\"\n",
    "\n",
    "os.makedirs(\"wiki_index\", exist_ok=True)\n",
    "ix = index.create_in(\"wiki_index\", schema)\n",
    "writer = ix.writer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) Adapt the helper functions\n",
    "\n",
    "Note the subtle changes.\n",
    "Please adapt the code below such as provided recursive parsing of the HTML (.html) files, indexing with the Whoosh API.\n",
    "Trust no code, verify all code segments.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visible(element):  # return those html elements that are visible as text \n",
    "    if element.parent.name in ['style', 'script', 'document', 'head', 'title']: #html tags\n",
    "        return False\n",
    "    elif re.match('<!--.*-->', str(element)): # html comments\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def pullBiota(soup):\n",
    "    \n",
    "    data = {}\n",
    "    cells=[]\n",
    "    \n",
    "    table = soup.find('table', class_='infobox biota') # extract infobox biota table\n",
    "\n",
    "    rows = table.find_all('tr') # extract rows\n",
    "    \n",
    "    for row in rows:\n",
    "        cells.append(row.find_all('td')) # creates list of lists of cells per row\n",
    "\n",
    "    cells=[x for x in cells if len(x)==2] # only retains rows with 2 cells each\n",
    "    \n",
    "    for i in cells:\n",
    "        data[i[0].get_text()] = i[1].get_text() # sets key to first cell and value to second cell\n",
    "    \n",
    "    return data\n",
    "\n",
    "\n",
    "def loadFile(writer, fname):\n",
    "    '''\n",
    "    Read file contents, load into database.\n",
    "    '''\n",
    "    with open(fname, 'r') as infile:\n",
    "        content=infile.read()   # read html content\n",
    "        \n",
    "        soup = BeautifulSoup(content, 'html.parser')\n",
    "        texts = soup.find_all(text=True)\n",
    "        \n",
    "        # Process all the visible text\n",
    "        visible_texts = filter(visible, texts)\n",
    "        \n",
    "        # TODO: Assemble all visible_texts into a content string\n",
    "        # Hint: Iterate over visible_texts line by line; remove newlines; create a concatenated string\n",
    "        processed_text = \"\"\n",
    "        \n",
    "        for line in visible_texts:\n",
    "            line = line.rstrip('\\n')\n",
    "            processed_text+=line\n",
    "        \n",
    "        \n",
    "        # TODO: Process the \"<table class=\"infobox biota\" ... </table> data\n",
    "        infotable = pullBiota(soup)\n",
    "        \n",
    "        # Write to the index\n",
    "        \n",
    "        writer.add_document(filename=fname,content=processed_text,kingdom=infotable.get('Kingdom:'),\n",
    "                            phylum=infotable.get('Phylum:'),an_class=infotable.get('Class:'),an_order=infotable.get('Order:'),\n",
    "                            family=infotable.get('Family:'),subfamily=infotable.get('Subfamily:'),\n",
    "                            genus=infotable.get('Genus:'))\n",
    "        \n",
    "        print(\"Indexed: \", fname)\n",
    "\n",
    "def processFolder(writer,folder):\n",
    "    '''\n",
    "    Process a folder for files and subfolders\n",
    "    '''\n",
    "    print('Processing folder: ',folder)\n",
    "    for root, dirs, files in os.walk(folder):\n",
    "        print(\"root = \", root)\n",
    "        # Process Files\n",
    "        for file in files:\n",
    "            if file.endswith(\".html\"):\n",
    "                filename = os.path.join(root, file)\n",
    "                print('Processing File:',filename)\n",
    "                loadFile(writer,filename)\n",
    "            else:\n",
    "                print(\"Unhandled File\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) Parse with our defined functions in place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing folder:  /dsa/data/all_datasets/en.wikipedia.org/wiki\n",
      "root =  /dsa/data/all_datasets/en.wikipedia.org/wiki\n",
      "Processing File: /dsa/data/all_datasets/en.wikipedia.org/wiki/Acris.html\n",
      "Indexed:  /dsa/data/all_datasets/en.wikipedia.org/wiki/Acris.html\n",
      "Processing File: /dsa/data/all_datasets/en.wikipedia.org/wiki/Anotheca.html\n",
      "Indexed:  /dsa/data/all_datasets/en.wikipedia.org/wiki/Anotheca.html\n",
      "Processing File: /dsa/data/all_datasets/en.wikipedia.org/wiki/Aparasphenodon.html\n",
      "Indexed:  /dsa/data/all_datasets/en.wikipedia.org/wiki/Aparasphenodon.html\n",
      "Processing File: /dsa/data/all_datasets/en.wikipedia.org/wiki/Aplastodiscus.html\n",
      "Indexed:  /dsa/data/all_datasets/en.wikipedia.org/wiki/Aplastodiscus.html\n",
      "Processing File: /dsa/data/all_datasets/en.wikipedia.org/wiki/Argenteohyla.html\n",
      "Indexed:  /dsa/data/all_datasets/en.wikipedia.org/wiki/Argenteohyla.html\n",
      "Processing File: /dsa/data/all_datasets/en.wikipedia.org/wiki/Bokermannohyla.html\n",
      "Indexed:  /dsa/data/all_datasets/en.wikipedia.org/wiki/Bokermannohyla.html\n",
      "Processing File: /dsa/data/all_datasets/en.wikipedia.org/wiki/Bromeliohyla.html\n",
      "Indexed:  /dsa/data/all_datasets/en.wikipedia.org/wiki/Bromeliohyla.html\n",
      "Processing File: /dsa/data/all_datasets/en.wikipedia.org/wiki/Charadrahyla.html\n",
      "Indexed:  /dsa/data/all_datasets/en.wikipedia.org/wiki/Charadrahyla.html\n",
      "Processing File: /dsa/data/all_datasets/en.wikipedia.org/wiki/Corythomantis.html\n",
      "Indexed:  /dsa/data/all_datasets/en.wikipedia.org/wiki/Corythomantis.html\n",
      "Processing File: /dsa/data/all_datasets/en.wikipedia.org/wiki/Dendropsophus.html\n",
      "Indexed:  /dsa/data/all_datasets/en.wikipedia.org/wiki/Dendropsophus.html\n",
      "Processing File: /dsa/data/all_datasets/en.wikipedia.org/wiki/Duellmanohyla.html\n",
      "Indexed:  /dsa/data/all_datasets/en.wikipedia.org/wiki/Duellmanohyla.html\n",
      "Processing File: /dsa/data/all_datasets/en.wikipedia.org/wiki/Ecnomiohyla.html\n",
      "Indexed:  /dsa/data/all_datasets/en.wikipedia.org/wiki/Ecnomiohyla.html\n",
      "Processing File: /dsa/data/all_datasets/en.wikipedia.org/wiki/Exerodonta.html\n",
      "Indexed:  /dsa/data/all_datasets/en.wikipedia.org/wiki/Exerodonta.html\n",
      "Processing File: /dsa/data/all_datasets/en.wikipedia.org/wiki/Hyla.html\n",
      "Indexed:  /dsa/data/all_datasets/en.wikipedia.org/wiki/Hyla.html\n",
      "Processing File: /dsa/data/all_datasets/en.wikipedia.org/wiki/Hylidae.html\n",
      "Indexed:  /dsa/data/all_datasets/en.wikipedia.org/wiki/Hylidae.html\n",
      "Processing File: /dsa/data/all_datasets/en.wikipedia.org/wiki/Hylinae.html\n",
      "Indexed:  /dsa/data/all_datasets/en.wikipedia.org/wiki/Hylinae.html\n",
      "Processing File: /dsa/data/all_datasets/en.wikipedia.org/wiki/Hyloscirtus.html\n",
      "Indexed:  /dsa/data/all_datasets/en.wikipedia.org/wiki/Hyloscirtus.html\n",
      "Processing File: /dsa/data/all_datasets/en.wikipedia.org/wiki/Hypsiboas.html\n",
      "Indexed:  /dsa/data/all_datasets/en.wikipedia.org/wiki/Hypsiboas.html\n",
      "Processing File: /dsa/data/all_datasets/en.wikipedia.org/wiki/Isthmohyla.html\n",
      "Indexed:  /dsa/data/all_datasets/en.wikipedia.org/wiki/Isthmohyla.html\n",
      "Processing File: /dsa/data/all_datasets/en.wikipedia.org/wiki/Itapotihyla.html\n",
      "Indexed:  /dsa/data/all_datasets/en.wikipedia.org/wiki/Itapotihyla.html\n",
      "Processing File: /dsa/data/all_datasets/en.wikipedia.org/wiki/Lysapsus.html\n",
      "Indexed:  /dsa/data/all_datasets/en.wikipedia.org/wiki/Lysapsus.html\n",
      "Processing File: /dsa/data/all_datasets/en.wikipedia.org/wiki/Megastomatohyla.html\n",
      "Indexed:  /dsa/data/all_datasets/en.wikipedia.org/wiki/Megastomatohyla.html\n",
      "Processing File: /dsa/data/all_datasets/en.wikipedia.org/wiki/Myersiohyla.html\n",
      "Indexed:  /dsa/data/all_datasets/en.wikipedia.org/wiki/Myersiohyla.html\n",
      "Processing File: /dsa/data/all_datasets/en.wikipedia.org/wiki/Nyctimantis.html\n",
      "Indexed:  /dsa/data/all_datasets/en.wikipedia.org/wiki/Nyctimantis.html\n",
      "Processing File: /dsa/data/all_datasets/en.wikipedia.org/wiki/Osteocephalus.html\n",
      "Indexed:  /dsa/data/all_datasets/en.wikipedia.org/wiki/Osteocephalus.html\n",
      "Processing File: /dsa/data/all_datasets/en.wikipedia.org/wiki/Osteopilus.html\n",
      "Indexed:  /dsa/data/all_datasets/en.wikipedia.org/wiki/Osteopilus.html\n",
      "Processing File: /dsa/data/all_datasets/en.wikipedia.org/wiki/Phyllodytes.html\n",
      "Indexed:  /dsa/data/all_datasets/en.wikipedia.org/wiki/Phyllodytes.html\n",
      "Processing File: /dsa/data/all_datasets/en.wikipedia.org/wiki/Phytotriades.html\n",
      "Indexed:  /dsa/data/all_datasets/en.wikipedia.org/wiki/Phytotriades.html\n",
      "Processing File: /dsa/data/all_datasets/en.wikipedia.org/wiki/Plectrohyla.html\n",
      "Indexed:  /dsa/data/all_datasets/en.wikipedia.org/wiki/Plectrohyla.html\n",
      "Processing File: /dsa/data/all_datasets/en.wikipedia.org/wiki/Pseudacris.html\n",
      "Indexed:  /dsa/data/all_datasets/en.wikipedia.org/wiki/Pseudacris.html\n",
      "Processing File: /dsa/data/all_datasets/en.wikipedia.org/wiki/Pseudis.html\n",
      "Indexed:  /dsa/data/all_datasets/en.wikipedia.org/wiki/Pseudis.html\n",
      "Processing File: /dsa/data/all_datasets/en.wikipedia.org/wiki/Ptychohyla.html\n",
      "Indexed:  /dsa/data/all_datasets/en.wikipedia.org/wiki/Ptychohyla.html\n",
      "Processing File: /dsa/data/all_datasets/en.wikipedia.org/wiki/Scarthyla.html\n",
      "Indexed:  /dsa/data/all_datasets/en.wikipedia.org/wiki/Scarthyla.html\n",
      "Processing File: /dsa/data/all_datasets/en.wikipedia.org/wiki/Scinax.html\n",
      "Indexed:  /dsa/data/all_datasets/en.wikipedia.org/wiki/Scinax.html\n",
      "Processing File: /dsa/data/all_datasets/en.wikipedia.org/wiki/Smilisca.html\n",
      "Indexed:  /dsa/data/all_datasets/en.wikipedia.org/wiki/Smilisca.html\n",
      "Processing File: /dsa/data/all_datasets/en.wikipedia.org/wiki/Sphaenorhynchus.html\n",
      "Indexed:  /dsa/data/all_datasets/en.wikipedia.org/wiki/Sphaenorhynchus.html\n",
      "Processing File: /dsa/data/all_datasets/en.wikipedia.org/wiki/Tepuihyla.html\n",
      "Indexed:  /dsa/data/all_datasets/en.wikipedia.org/wiki/Tepuihyla.html\n",
      "Processing File: /dsa/data/all_datasets/en.wikipedia.org/wiki/Tlalocohyla.html\n",
      "Indexed:  /dsa/data/all_datasets/en.wikipedia.org/wiki/Tlalocohyla.html\n",
      "Processing File: /dsa/data/all_datasets/en.wikipedia.org/wiki/Trachycephalus.html\n",
      "Indexed:  /dsa/data/all_datasets/en.wikipedia.org/wiki/Trachycephalus.html\n",
      "Processing File: /dsa/data/all_datasets/en.wikipedia.org/wiki/Triprion.html\n",
      "Indexed:  /dsa/data/all_datasets/en.wikipedia.org/wiki/Triprion.html\n",
      "Processing File: /dsa/data/all_datasets/en.wikipedia.org/wiki/Xenohyla.html\n",
      "Indexed:  /dsa/data/all_datasets/en.wikipedia.org/wiki/Xenohyla.html\n"
     ]
    }
   ],
   "source": [
    "# Start processing the folder and commit the work\n",
    "# ---------------------------------------------------\n",
    "\n",
    "processFolder(writer,\"/dsa/data/all_datasets/en.wikipedia.org/wiki\")\n",
    "writer.commit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "<a id='search_me' ></a>\n",
    "\n",
    "### 5) Executing Queries\n",
    "\n",
    "Read: \n",
    "  http://whoosh.readthedocs.io/en/latest/searching.html\n",
    "  \n",
    "Previously, we hard-coded query strings into the code cells.\n",
    "\n",
    "Now, use the `input()` function collect a query string from the user. \n",
    "Then execute the search. For this task, focus only on the `content` field. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter a string: frog\n",
      "/dsa/data/all_datasets/en.wikipedia.org/wiki/Smilisca.html 1.9813945223168978 0\n",
      "/dsa/data/all_datasets/en.wikipedia.org/wiki/Sphaenorhynchus.html 1.970012642899165 1\n",
      "/dsa/data/all_datasets/en.wikipedia.org/wiki/Hylidae.html 1.9345465722677746 2\n",
      "/dsa/data/all_datasets/en.wikipedia.org/wiki/Pseudis.html 1.8908179954008726 3\n",
      "/dsa/data/all_datasets/en.wikipedia.org/wiki/Osteopilus.html 1.890020489001432 4\n",
      "/dsa/data/all_datasets/en.wikipedia.org/wiki/Ptychohyla.html 1.890020489001432 5\n",
      "/dsa/data/all_datasets/en.wikipedia.org/wiki/Pseudacris.html 1.8432853499981452 6\n",
      "/dsa/data/all_datasets/en.wikipedia.org/wiki/Ecnomiohyla.html 1.7583032681359634 7\n",
      "/dsa/data/all_datasets/en.wikipedia.org/wiki/Ecnomiohyla.html 1.7583032681359634 8\n",
      "/dsa/data/all_datasets/en.wikipedia.org/wiki/Hyla.html 1.747022624013118 9\n"
     ]
    }
   ],
   "source": [
    "from whoosh.qparser import QueryParser\n",
    "\n",
    "# Write your code below this comment:\n",
    "# --------------------------------------\n",
    "\n",
    "qp = QueryParser(\"content\", schema=ix.schema)\n",
    "\n",
    "query = input(\"Enter a string: \")\n",
    "\n",
    "q = qp.parse(query)\n",
    "\n",
    "with ix.searcher() as s:\n",
    "    results = s.search(q)\n",
    "    for hit in results:\n",
    "        print(hit['filename'], hit.score, hit.rank)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6) Write two example queries to ensure you can search the index \n",
    "\n",
    "That is, make sure you can search on the fields you added to the index from the infobox biota table.\n",
    "\n",
    "```HTML\n",
    "<table class=\"infobox biota\" ... </table>\n",
    "```\n",
    "For this search, we will ignore `content` field and search over the other fields. We can use `MultifieldParser` to specify the fields of our interest. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/dsa/data/all_datasets/en.wikipedia.org/wiki/Acris.html 0.9819814944973216 0\n",
      "/dsa/data/all_datasets/en.wikipedia.org/wiki/Anotheca.html 0.9819814944973216 1\n",
      "/dsa/data/all_datasets/en.wikipedia.org/wiki/Aparasphenodon.html 0.9819814944973216 2\n",
      "/dsa/data/all_datasets/en.wikipedia.org/wiki/Aplastodiscus.html 0.9819814944973216 3\n",
      "/dsa/data/all_datasets/en.wikipedia.org/wiki/Argenteohyla.html 0.9819814944973216 4\n",
      "/dsa/data/all_datasets/en.wikipedia.org/wiki/Bokermannohyla.html 0.9819814944973216 5\n",
      "/dsa/data/all_datasets/en.wikipedia.org/wiki/Bromeliohyla.html 0.9819814944973216 6\n",
      "/dsa/data/all_datasets/en.wikipedia.org/wiki/Charadrahyla.html 0.9819814944973216 7\n",
      "/dsa/data/all_datasets/en.wikipedia.org/wiki/Corythomantis.html 0.9819814944973216 8\n",
      "/dsa/data/all_datasets/en.wikipedia.org/wiki/Dendropsophus.html 0.9819814944973216 9\n"
     ]
    }
   ],
   "source": [
    "# Write your code below this comment:\n",
    "# --------------------------------------\n",
    "import whoosh\n",
    "from whoosh.qparser import MultifieldParser\n",
    "from whoosh import *\n",
    "\n",
    "# OMIT CONTENT\n",
    "qp = MultifieldParser([\"kingdom\",\"phylum\",\"an_class\",\"an_order\",\"family\",\"subfamily\",\"genus\"], \n",
    "                      schema=ix.schema, group=qparser.OrGroup)  \n",
    "\n",
    "q = qp.parse(\"Animalia\") # searches the hierarchies for \"Animalia\"\n",
    "\n",
    "with ix.searcher() as s:\n",
    "    results = s.search(q)\n",
    "    for hit in results:\n",
    "        print(hit['filename'], hit.score, hit.rank)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/dsa/data/all_datasets/en.wikipedia.org/wiki/Acris.html 0.9819814944973216 0\n",
      "/dsa/data/all_datasets/en.wikipedia.org/wiki/Anotheca.html 0.9819814944973216 1\n",
      "/dsa/data/all_datasets/en.wikipedia.org/wiki/Aparasphenodon.html 0.9819814944973216 2\n",
      "/dsa/data/all_datasets/en.wikipedia.org/wiki/Aplastodiscus.html 0.9819814944973216 3\n",
      "/dsa/data/all_datasets/en.wikipedia.org/wiki/Argenteohyla.html 0.9819814944973216 4\n",
      "/dsa/data/all_datasets/en.wikipedia.org/wiki/Bokermannohyla.html 0.9819814944973216 5\n",
      "/dsa/data/all_datasets/en.wikipedia.org/wiki/Bromeliohyla.html 0.9819814944973216 6\n",
      "/dsa/data/all_datasets/en.wikipedia.org/wiki/Charadrahyla.html 0.9819814944973216 7\n",
      "/dsa/data/all_datasets/en.wikipedia.org/wiki/Corythomantis.html 0.9819814944973216 8\n",
      "/dsa/data/all_datasets/en.wikipedia.org/wiki/Dendropsophus.html 0.9819814944973216 9\n"
     ]
    }
   ],
   "source": [
    "# Write your code below this comment:\n",
    "# --------------------------------------\n",
    "\n",
    "# OMIT CONTENT\n",
    "qp = MultifieldParser([\"kingdom\",\"phylum\",\"an_class\",\"an_order\",\"family\",\"subfamily\",\"genus\"], \n",
    "                      schema=ix.schema, group=qparser.OrGroup)\n",
    "\n",
    "q = qp.parse(\"Amphibia\") # searches the hierarchies for \"Amphibia\"\n",
    "\n",
    "with ix.searcher() as s:\n",
    "    results = s.search(q)\n",
    "    for hit in results:\n",
    "        print(hit['filename'], hit.score, hit.rank)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SAVE YOUR NOTEBOOK WITH ALL EXECUTED CELLS\n",
    "# Then, `File > Close and Halt`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
